{{- if and .Values.monitoring.enabled .Values.monitoring.prometheusRules.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "gitops-lifecycle-management.fullname" . }}-alerts
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gitops-lifecycle-management.labels" . | nindent 4 }}
    app.kubernetes.io/component: monitoring
  annotations:
    {{- include "gitops-lifecycle-management.annotations" . | nindent 4 }}
spec:
  groups:
    - name: gitops-lifecycle-management.cleanup
      interval: {{ .Values.monitoring.prometheusRules.evaluationInterval | default "30s" }}
      rules:
        # Cleanup controller health
        - alert: GitOpsCleanupControllerDown
          expr: up{job="gitops-lifecycle-management-cleanup-metrics"} == 0
          for: {{ .Values.monitoring.prometheusRules.alertFor.controllerDown | default "5m" }}
          labels:
            severity: critical
            component: cleanup-controller
          annotations:
            summary: "GitOps cleanup controller is down"
            description: "The GitOps lifecycle management cleanup controller has been down for more than 5 minutes."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/cleanup-controller-down"
        
        # High cleanup failure rate
        - alert: GitOpsCleanupHighFailureRate
          expr: |
            (
              rate(gitops_cleanup_errors_total[5m]) / 
              rate(gitops_cleanup_cycle_duration_seconds_count[5m])
            ) > {{ .Values.monitoring.prometheusRules.thresholds.cleanupFailureRate | default "0.1" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.highFailureRate | default "10m" }}
          labels:
            severity: warning
            component: cleanup-controller
          annotations:
            summary: "High cleanup failure rate detected"
            description: "Cleanup operations are failing at a rate of {{ "{{ $value | humanizePercentage }}" }} over the last 5 minutes."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/high-cleanup-failure-rate"
        
        # Cleanup cycle duration too long
        - alert: GitOpsCleanupCycleTooLong
          expr: |
            histogram_quantile(0.95, rate(gitops_cleanup_cycle_duration_seconds_bucket[5m])) > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxCleanupDuration | default "300" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.longCycle | default "15m" }}
          labels:
            severity: warning
            component: cleanup-controller
          annotations:
            summary: "Cleanup cycles taking too long"
            description: "95th percentile of cleanup cycle duration is {{ "{{ $value | humanizeDuration }}" }}, which exceeds the threshold."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/cleanup-cycle-too-long"
        
        # Too many jobs being cleaned up
        - alert: GitOpsExcessiveJobCleanup
          expr: |
            rate(gitops_cleanup_jobs_cleaned_total[1h]) > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxJobCleanupRate | default "10" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.excessiveCleanup | default "30m" }}
          labels:
            severity: warning
            component: cleanup-controller
          annotations:
            summary: "Excessive job cleanup detected"
            description: "Jobs are being cleaned up at a rate of {{ "{{ $value | humanize }}" }} per hour, which may indicate a problem with job completion."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/excessive-job-cleanup"
        
        # Stuck resources accumulating
        - alert: GitOpsStuckResourcesAccumulating
          expr: |
            increase(gitops_cleanup_stuck_resources_cleaned_total[1h]) > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxStuckResourcesPerHour | default "5" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.stuckResources | default "1h" }}
          labels:
            severity: warning
            component: cleanup-controller
          annotations:
            summary: "Stuck resources accumulating"
            description: "{{ "{{ $value }}" }} stuck resources have been cleaned up in the last hour, indicating potential issues with resource lifecycle management."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/stuck-resources-accumulating"

    - name: gitops-lifecycle-management.retry
      interval: {{ .Values.monitoring.prometheusRules.evaluationInterval | default "30s" }}
      rules:
        # High retry failure rate
        - alert: GitOpsRetryHighFailureRate
          expr: |
            (
              rate(gitops_retry_failure_total[5m]) / 
              (rate(gitops_retry_success_total[5m]) + rate(gitops_retry_failure_total[5m]))
            ) > {{ .Values.monitoring.prometheusRules.thresholds.retryFailureRate | default "0.2" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.retryFailure | default "10m" }}
          labels:
            severity: warning
            component: retry-mechanism
          annotations:
            summary: "High retry failure rate detected"
            description: "Retry operations are failing at a rate of {{ "{{ $value | humanizePercentage }}" }} for operation {{ "{{ $labels.operation }}" }}."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/high-retry-failure-rate"
        
        # Excessive retry attempts
        - alert: GitOpsExcessiveRetryAttempts
          expr: |
            rate(gitops_retry_attempts_total[5m]) > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxRetryRate | default "5" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.excessiveRetries | default "15m" }}
          labels:
            severity: warning
            component: retry-mechanism
          annotations:
            summary: "Excessive retry attempts detected"
            description: "Retry attempts for operation {{ "{{ $labels.operation }}" }} are occurring at {{ "{{ $value | humanize }}" }} per second."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/excessive-retry-attempts"
        
        # Long retry duration
        - alert: GitOpsRetryDurationTooLong
          expr: |
            histogram_quantile(0.95, rate(gitops_retry_duration_seconds_bucket[5m])) > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxRetryDuration | default "600" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.longRetry | default "10m" }}
          labels:
            severity: warning
            component: retry-mechanism
          annotations:
            summary: "Retry operations taking too long"
            description: "95th percentile of retry duration for operation {{ "{{ $labels.operation }}" }} is {{ "{{ $value | humanizeDuration }}" }}."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/retry-duration-too-long"

    - name: gitops-lifecycle-management.service-discovery
      interval: {{ .Values.monitoring.prometheusRules.evaluationInterval | default "30s" }}
      rules:
        # Service discovery controller health
        - alert: GitOpsServiceDiscoveryControllerDown
          expr: up{job="gitops-lifecycle-management-service-discovery-metrics"} == 0
          for: {{ .Values.monitoring.prometheusRules.alertFor.controllerDown | default "5m" }}
          labels:
            severity: critical
            component: service-discovery-controller
          annotations:
            summary: "GitOps service discovery controller is down"
            description: "The GitOps lifecycle management service discovery controller has been down for more than 5 minutes."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/service-discovery-controller-down"
        
        # ProxyConfig resources stuck
        - alert: GitOpsProxyConfigStuck
          expr: |
            gitops_proxyconfig_phase{phase="Pending"} > 0 and 
            time() - gitops_proxyconfig_last_reconciled > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxProxyConfigPendingTime | default "600" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.stuckProxyConfig | default "10m" }}
          labels:
            severity: warning
            component: service-discovery-controller
          annotations:
            summary: "ProxyConfig resources stuck in pending state"
            description: "ProxyConfig {{ "{{ $labels.name }}" }} in namespace {{ "{{ $labels.namespace }}" }} has been pending for more than 10 minutes."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/proxyconfig-stuck"

    - name: gitops-lifecycle-management.hooks
      interval: {{ .Values.monitoring.prometheusRules.evaluationInterval | default "30s" }}
      rules:
        # Hook failures
        - alert: GitOpsHookFailures
          expr: |
            increase(kube_job_status_failed{job_name=~".*gitops-lifecycle-management.*"}[1h]) > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxHookFailuresPerHour | default "3" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.hookFailures | default "5m" }}
          labels:
            severity: warning
            component: helm-hooks
          annotations:
            summary: "GitOps lifecycle management hook failures"
            description: "{{ "{{ $value }}" }} GitOps lifecycle management hooks have failed in the last hour."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/hook-failures"
        
        # Hook duration too long
        - alert: GitOpsHookDurationTooLong
          expr: |
            kube_job_status_active{job_name=~".*gitops-lifecycle-management.*"} == 1 and
            time() - kube_job_status_start_time{job_name=~".*gitops-lifecycle-management.*"} > 
            {{ .Values.monitoring.prometheusRules.thresholds.maxHookDuration | default "900" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.longHook | default "5m" }}
          labels:
            severity: warning
            component: helm-hooks
          annotations:
            summary: "GitOps hook running too long"
            description: "Hook job {{ "{{ $labels.job_name }}" }} has been running for more than {{ "{{ .Values.monitoring.prometheusRules.thresholds.maxHookDuration | default \"900\" | humanizeDuration }}" }}."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/hook-duration-too-long"

    - name: gitops-lifecycle-management.resources
      interval: {{ .Values.monitoring.prometheusRules.evaluationInterval | default "30s" }}
      rules:
        # High memory usage
        - alert: GitOpsControllerHighMemoryUsage
          expr: |
            (
              container_memory_working_set_bytes{container=~".*gitops-lifecycle-management.*"} /
              container_spec_memory_limit_bytes{container=~".*gitops-lifecycle-management.*"}
            ) > {{ .Values.monitoring.prometheusRules.thresholds.memoryUsageThreshold | default "0.8" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.highMemoryUsage | default "10m" }}
          labels:
            severity: warning
            component: resource-usage
          annotations:
            summary: "GitOps controller high memory usage"
            description: "Container {{ "{{ $labels.container }}" }} in pod {{ "{{ $labels.pod }}" }} is using {{ "{{ $value | humanizePercentage }}" }} of its memory limit."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/high-memory-usage"
        
        # High CPU usage
        - alert: GitOpsControllerHighCPUUsage
          expr: |
            (
              rate(container_cpu_usage_seconds_total{container=~".*gitops-lifecycle-management.*"}[5m]) /
              container_spec_cpu_quota{container=~".*gitops-lifecycle-management.*"} *
              container_spec_cpu_period{container=~".*gitops-lifecycle-management.*"}
            ) > {{ .Values.monitoring.prometheusRules.thresholds.cpuUsageThreshold | default "0.8" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.highCPUUsage | default "10m" }}
          labels:
            severity: warning
            component: resource-usage
          annotations:
            summary: "GitOps controller high CPU usage"
            description: "Container {{ "{{ $labels.container }}" }} in pod {{ "{{ $labels.pod }}" }} is using {{ "{{ $value | humanizePercentage }}" }} of its CPU limit."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/high-cpu-usage"
        
        # Pod restart rate too high
        - alert: GitOpsControllerHighRestartRate
          expr: |
            rate(kube_pod_container_status_restarts_total{container=~".*gitops-lifecycle-management.*"}[1h]) >
            {{ .Values.monitoring.prometheusRules.thresholds.maxRestartRate | default "0.1" }}
          for: {{ .Values.monitoring.prometheusRules.alertFor.highRestartRate | default "15m" }}
          labels:
            severity: warning
            component: pod-health
          annotations:
            summary: "GitOps controller high restart rate"
            description: "Container {{ "{{ $labels.container }}" }} in pod {{ "{{ $labels.pod }}" }} is restarting at {{ "{{ $value | humanize }}" }} times per hour."
            runbook_url: "{{ .Values.monitoring.prometheusRules.runbookUrl }}/high-restart-rate"
{{- end }}