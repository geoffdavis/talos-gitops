apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  timeout: 30m
  install:
    timeout: 30m
    remediation:
      retries: 3
  upgrade:
    timeout: 30m
    remediation:
      retries: 3
      remediateLastFailure: true
    cleanupOnFail: true
  rollback:
    timeout: 20m
    cleanupOnFail: true
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "75.15.0"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
      interval: 12h
  values:
    alertmanager:
      enabled: true
      service:
        type: LoadBalancer
        annotations:
          io.cilium/lb-ipam-pool: "bgp-default"
        labels:
          io.cilium/lb-ipam-pool: "bgp-default"

    prometheus:
      service:
        type: ClusterIP
        # Removed LoadBalancer configuration - access via kubectl port-forward only
        # annotations:
        #   io.cilium/lb-ipam-pool: "bgp-default"
        # labels:
        #   io.cilium/lb-ipam-pool: "bgp-default"

      prometheusSpec:
        storageSpec:
          volumeClaimTemplate:
            metadata:
              labels:
                backup-tier: "critical"
                backup-group: "monitoring"
                app: "prometheus"
            spec:
              storageClassName: longhorn
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi

        nodeSelector:
          kubernetes.io/os: linux

        tolerations:
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
            effect: NoSchedule
          - key: node-role.kubernetes.io/master
            operator: Exists
            effect: NoSchedule

    grafana:
      service:
        type: LoadBalancer
        annotations:
          io.cilium/lb-ipam-pool: "bgp-default"
        labels:
          io.cilium/lb-ipam-pool: "bgp-default"

      persistence:
        enabled: true
        storageClassName: longhorn
        size: 10Gi
        annotations:
          backup-tier: "critical"
          backup-group: "monitoring"
          app: "grafana"

      # Native OIDC Authentication Configuration
      env:
        GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
        GF_AUTH_GENERIC_OAUTH_NAME: "Authentik"
        GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "grafana"
        GF_AUTH_GENERIC_OAUTH_SCOPES: "openid profile email"
        GF_AUTH_GENERIC_OAUTH_AUTH_URL: "https://authentik.k8s.home.geoffdavis.com/application/o/authorize/"
        GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "https://authentik.k8s.home.geoffdavis.com/application/o/token/"
        GF_AUTH_GENERIC_OAUTH_API_URL: "https://authentik.k8s.home.geoffdavis.com/application/o/userinfo/"
        GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
        GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN: "false"
        GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'Grafana Editors') && 'Editor' || 'Viewer'"
        GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_STRICT: "false"
        GF_AUTH_GENERIC_OAUTH_ALLOW_ASSIGN_GRAFANA_ADMIN: "true"
        GF_AUTH_GENERIC_OAUTH_SKIP_ORG_ROLE_SYNC: "false"
        # Disable anonymous access when OIDC is enabled
        GF_AUTH_ANONYMOUS_ENABLED: "false"
        # Security settings
        GF_AUTH_GENERIC_OAUTH_TLS_SKIP_VERIFY_INSECURE: "false"
        GF_AUTH_GENERIC_OAUTH_USE_PKCE: "true"

      envFromSecrets:
        - name: grafana-oidc-secret
          keys:
            - key: client-secret
              name: GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET

      # Ingress configuration for direct access (bypassing proxy)
      ingress:
        enabled: true
        ingressClassName: nginx-internal
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-prod
          nginx.ingress.kubernetes.io/ssl-redirect: "true"
          nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        hosts:
          - grafana.k8s.home.geoffdavis.com
        tls:
          - secretName: grafana-tls # pragma: allowlist secret
            hosts:
              - grafana.k8s.home.geoffdavis.com

      # Fix pod security policy violations
      securityContext:
        runAsNonRoot: true
        runAsUser: 472
        runAsGroup: 472
        fsGroup: 472
        seccompProfile:
          type: RuntimeDefault

      containerSecurityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 472
        runAsGroup: 472
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault

      # Disable the problematic init container and use fsGroup instead
      initChownData:
        enabled: false

      nodeSelector:
        kubernetes.io/os: linux

      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
