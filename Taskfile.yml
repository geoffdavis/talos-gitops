version: "3"

dotenv: [".env"]

vars:
  CLUSTER_NAME: home-ops
  CONTROL_PLANE_SUBNET: 172.29.51.0/24
  # renovate: datasource=github-releases depName=siderolabs/talos
  TALOS_VERSION: v1.10.5
  # renovate: datasource=github-releases depName=kubernetes/kubernetes
  KUBERNETES_VERSION: v1.31.1
  CLUSTER_ENDPOINT: https://172.29.51.10:6443
  NODE_1_IP: 172.29.51.11
  NODE_2_IP: 172.29.51.12
  NODE_3_IP: 172.29.51.13
  TALOSCONFIG: talos/generated/talosconfig

tasks:
  # Bootstrap tasks
  bootstrap:secrets:
    desc: Bootstrap secrets from 1Password (requires OP_ACCOUNT environment variable)
    cmds:
      - ./scripts/bootstrap-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: "OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)"

  bootstrap:k8s-secrets:
    desc: Bootstrap Kubernetes secrets from 1Password (for existing cluster)
    cmds:
      - ./scripts/bootstrap-k8s-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: "OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)"
      - sh: "mise exec -- kubectl get namespaces &> /dev/null"
        msg: "Kubernetes cluster must be accessible"

  bootstrap:1password-secrets:
    desc: Bootstrap 1Password Connect secrets for fresh cluster (requires OP_ACCOUNT environment variable)
    cmds:
      - ./scripts/bootstrap-1password-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: "OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)"

  bootstrap:validate-1password-secrets:
    desc: Validate 1Password Connect secrets were created correctly
    cmds:
      - ./scripts/validate-1password-secrets.sh

  bootstrap:cluster:
    desc: Bootstrap the entire cluster from scratch with integrated LLDPD configuration (legacy - use bootstrap:phased for new approach)
    deps: [bootstrap:secrets, talos:generate-config]
    cmds:
      - mise exec -- task talos:apply-config
      - mise exec -- task talos:bootstrap
      - mise exec -- task bootstrap:1password-secrets
      - mise exec -- task bootstrap:validate-1password-secrets
      - mise exec -- task flux:bootstrap
      - mise exec -- task apps:deploy-core

  # Bootstrap tasks moved to taskfiles/bootstrap.yml to avoid duplication
  # Phase validation tasks moved to taskfiles/validation.yml to avoid duplication

  # Talos tasks
  talos:generate-schematic:
    desc: Generate custom Talos schematic with extensions via Image Factory
    cmds:
      - ./scripts/generate-talos-schematic.sh
    preconditions:
      - sh: "command -v curl"
        msg: "curl is required for Image Factory API"
      - sh: "command -v jq"
        msg: "jq is required for JSON processing"

  talos:update-installer-images:
    desc: Update machine configurations to use custom installer images
    deps: [talos:generate-schematic]
    cmds:
      - ./scripts/update-installer-images.sh
    preconditions:
      - sh: "[ -f talos/generated/schematic-id.txt ]"
        msg: "Schematic ID file not found. Run task talos:generate-schematic first."

  talos:restore-secrets:
    desc: Restore Talos secrets from the correct 1Password entry (talos - home-ops)
    preconditions:
      - sh: '[ -n "${OP_ACCOUNT:-}" ]'
        msg: "OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)"
    cmds:
      - echo "Restoring Talos secrets from 1Password 'talos - {{.CLUSTER_NAME}}' entry..."
      - |
        op --account {{.OP_ACCOUNT}} item get "talos - {{.CLUSTER_NAME}}" --vault="Automation" --format json | jq -r '{
          cluster: {
            id: .fields[] | select(.label == "cluster_id") | .value,
            secret: .fields[] | select(.label == "cluster_secret") | .value
          },
          secrets: {
            bootstraptoken: .fields[] | select(.label == "bootstraptoken") | .value,
            secretboxencryptionsecret: .fields[] | select(.label == "secretboxencryptionsecret") | .value
          },
          trustdinfo: {
            token: .fields[] | select(.label == "trustdinfo_token") | .value
          },
          certs: {
            etcd: {
              crt: .fields[] | select(.label == "cert_etcd_crt") | .value,
              key: .fields[] | select(.label == "cert_etcd_key") | .value
            },
            k8s: {
              crt: .fields[] | select(.label == "cert_k8s_crt") | .value,
              key: .fields[] | select(.label == "cert_k8s_key") | .value
            },
            k8saggregator: {
              crt: .fields[] | select(.label == "cert_k8saggregator_crt") | .value,
              key: .fields[] | select(.label == "cert_k8saggregator_key") | .value
            },
            k8sserviceaccount: {
              key: .fields[] | select(.label == "cert_k8sserviceaccount_key") | .value
            },
            os: {
              crt: .fields[] | select(.label == "cert_os_crt") | .value,
              key: .fields[] | select(.label == "cert_os_key") | .value
            }
          }
        }' | yq -P > talos/talsecret.yaml
      - echo "Secrets restored from 1Password"

  # talos:generate-config task moved to taskfiles/talos.yml to avoid duplication

  # talos:apply-config task moved to taskfiles/talos.yml to avoid duplication

  talos:apply-config-only:
    desc: Apply Talos configuration to nodes (without regenerating)
    env:
      TALOSCONFIG: talos/generated/talosconfig
    cmds:
      - echo "Applying Talos configuration to all control plane nodes..."
      - |
        # Try with talosconfig first, fall back to insecure if needed
        mise exec -- talosctl apply-config --nodes {{.NODE_1_IP}} --file talos/generated/home-ops-mini01.yaml || \
        mise exec -- talosctl apply-config --insecure --nodes {{.NODE_1_IP}} --file talos/generated/home-ops-mini01.yaml
      - |
        mise exec -- talosctl apply-config --nodes {{.NODE_2_IP}} --file talos/generated/home-ops-mini02.yaml || \
        mise exec -- talosctl apply-config --insecure --nodes {{.NODE_2_IP}} --file talos/generated/home-ops-mini02.yaml
      - |
        mise exec -- talosctl apply-config --nodes {{.NODE_3_IP}} --file talos/generated/home-ops-mini03.yaml || \
        mise exec -- talosctl apply-config --insecure --nodes {{.NODE_3_IP}} --file talos/generated/home-ops-mini03.yaml
      - echo "Configuration applied to all nodes"

  talos:apply-lldpd-config:
    desc: "[DEPRECATED] Apply LLDPD ExtensionServiceConfig - now integrated into main Talos config"
    env:
      TALOSCONFIG: talos/generated/talosconfig
    preconditions:
      - sh: "[ -f talos/manifests/lldpd-extension-config.yaml ]"
        msg: "LLDPD ExtensionServiceConfig manifest not found at talos/manifests/lldpd-extension-config.yaml"
    cmds:
      - 'echo "WARNING: DEPRECATED - LLDPD configuration is now integrated into the main Talos configuration"'
      - 'echo "WARNING: Use task talos:apply-config instead, which includes LLDPD configuration"'
      - 'echo "WARNING: This task is kept for backward compatibility only"'
      - echo ""
      - echo "Applying LLDPD ExtensionServiceConfig to all nodes..."
      - echo "This prevents LLDPD service startup failures that cause periodic reboots"
      - |
        mise exec -- talosctl patch machineconfig \
          --nodes {{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}} \
          --patch-file talos/manifests/lldpd-extension-config.yaml
      - echo "Waiting for LLDPD configuration to be applied..."
      - sleep 15
      - echo "Verifying LLDPD ExtensionServiceConfig is loaded..."
      - |
        echo "Checking ExtensionServiceConfig on all nodes..."
        mise exec -- talosctl get extensionserviceconfigs --nodes {{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}} || echo "ExtensionServiceConfigs will be visible after next reboot"
      - echo "‚úì LLDPD configuration applied successfully to all nodes"
      - echo "‚úì This prevents the periodic reboot issues caused by LLDPD service failures"

  talos:bootstrap:
    desc: Bootstrap Talos cluster with all control plane nodes
    env:
      TALOSCONFIG: talos/generated/talosconfig
    cmds:
      - echo "Bootstrapping first control plane node..."
      - mise exec -- talosctl bootstrap --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}}
      - echo "Waiting for cluster to initialize..."
      - sleep 30
      - echo "Getting kubeconfig from cluster..."
      - mise exec -- talosctl kubeconfig --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}} --force
      - echo "All three nodes are now functioning as control plane nodes"

  talos:convert-to-all-controlplane:
    desc: Convert existing cluster to all-control-plane setup
    deps: [talos:generate-config]
    cmds:
      - echo "Converting cluster to all-control-plane setup..."
      - echo "Applying control plane configuration to all nodes..."
      - mise exec -- talosctl apply-config --nodes {{.NODE_1_IP}} --file talos/generated/controlplane.yaml
      - mise exec -- talosctl apply-config --nodes {{.NODE_2_IP}} --file talos/generated/controlplane.yaml
      - mise exec -- talosctl apply-config --nodes {{.NODE_3_IP}} --file talos/generated/controlplane.yaml
      - echo "Waiting for nodes to restart and join as control planes..."
      - sleep 60
      - echo "Verifying cluster status..."
      - mise exec -- kubectl get nodes -o wide
      - echo "All nodes should now show as control planes with 'control-plane' role"

  talos:upgrade:
    desc: Upgrade Talos nodes
    cmds:
      - mise exec -- talosctl upgrade --nodes {{.NODE_1_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}
      - mise exec -- talosctl upgrade --nodes {{.NODE_2_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}
      - mise exec -- talosctl upgrade --nodes {{.NODE_3_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}

  talos:reboot:
    desc: Reboot specified nodes or all nodes (for USB detection)
    env:
      TALOSCONFIG: talos/generated/talosconfig
    vars:
      NODES: '{{.NODES | default "all"}}'
    cmds:
      - |
        if [ "{{.NODES}}" = "all" ]; then
          echo "Rebooting all nodes for USB device detection..."
          mise exec -- talosctl reboot --nodes {{.NODE_1_IP}}
          sleep 30
          mise exec -- talosctl reboot --nodes {{.NODE_2_IP}}
          sleep 30
          mise exec -- talosctl reboot --nodes {{.NODE_3_IP}}
          echo "All nodes rebooted. Wait for cluster to come back online."
        else
          echo "Rebooting node(s): {{.NODES}}"
          mise exec -- talosctl reboot --nodes {{.NODES}}
        fi

  talos:check-extensions:
    desc: Check if extensions are loaded on nodes
    cmds:
      - echo "Checking extensions on {{.NODE_1_IP}}..."
      - mise exec -- talosctl get extensions --nodes {{.NODE_1_IP}}
      - echo "Checking extensions on {{.NODE_2_IP}}..."
      - mise exec -- talosctl get extensions --nodes {{.NODE_2_IP}}
      - echo "Checking extensions on {{.NODE_3_IP}}..."
      - mise exec -- talosctl get extensions --nodes {{.NODE_3_IP}}

  # talos:recover-kubeconfig task moved to taskfiles/talos.yml to avoid duplication

  talos:fix-cilium:
    desc: Fix Cilium CNI after cluster issues (for Talos without kube-proxy)
    cmds:
      - echo "Redeploying Cilium with correct Talos configuration..."
      - mise exec -- task apps:deploy-cilium
      - echo "Checking Cilium status..."
      - mise exec -- kubectl get pods -n kube-system | grep cilium
      - echo "Nodes should become Ready within a few minutes"

  cluster:recover:
    desc: Complete cluster recovery process after power outage or certificate issues
    cmds:
      - echo "Starting complete cluster recovery process..."
      - echo "Step 1 - Ensuring environment is configured..."
      - 'test -f .env || (echo "ERROR: .env file missing. Copy from .env.example and set OP_ACCOUNT" && exit 1)'
      - source .env
      - echo "Step 2 - Restoring secrets from 1Password..."
      - mise exec -- task talos:restore-secrets
      - echo "Step 3 - Regenerating Talos configuration..."
      - mise exec -- task talos:generate-config
      - echo "Step 4 - Recovering kubeconfig..."
      - mise exec -- task talos:recover-kubeconfig
      - echo "Step 5 - Checking cluster status..."
      - mise exec -- kubectl get nodes || echo "Nodes not ready yet"
      - echo "Step 6 - Fixing Cilium if needed..."
      - mise exec -- task talos:fix-cilium
      - echo "Step 7 - Final status check..."
      - sleep 60
      - mise exec -- kubectl get nodes
      - echo "Recovery process complete. All nodes should be Ready."

  # Flux tasks
  flux:bootstrap:
    desc: Bootstrap Flux GitOps with 1Password integration
    preconditions:
      - sh: "command -v op"
        msg: "1Password CLI (op) is required to retrieve GitHub token"
    cmds:
      - echo "Bootstrapping Flux GitOps..."
      - |
        export GITHUB_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") && \
        mise exec -- flux bootstrap github \
          --owner=geoffdavis \
          --repository=talos-gitops \
          --branch=main \
          --path=clusters/home-ops \
          --personal=true

  flux:reconcile:
    desc: Force Flux reconciliation
    cmds:
      - mise exec -- flux reconcile source git flux-system
      - mise exec -- flux reconcile kustomization flux-system

  # Application deployment
  apps:deploy-core:
    desc: Deploy core applications (before Flux)
    cmds:
      - mise exec -- task apps:deploy-cilium
      - mise exec -- task apps:deploy-external-secrets
      - mise exec -- task apps:deploy-onepassword-connect
      - mise exec -- task apps:deploy-longhorn

  apps:deploy-cilium:
    desc: Deploy Cilium CNI for Talos (with LoadBalancer IPAM and L2 announcements)
    cmds:
      - mise exec -- helm repo add cilium https://helm.cilium.io/ || true
      - mise exec -- helm repo update
      - |
        mise exec -- helm upgrade --install cilium cilium/cilium \
            --version 1.17.6 \
            --namespace kube-system \
            --set ipam.mode=cluster-pool \
            --set ipam.operator.clusterPoolIPv4PodCIDRList="10.244.0.0/16" \
            --set ipam.operator.clusterPoolIPv4MaskSize=24 \
            --set kubeProxyReplacement=true \
            --set securityContext.privileged=true \
            --set cni.install=true \
            --set cni.exclusive=false \
            --set hubble.enabled=true \
            --set hubble.relay.enabled=true \
            --set hubble.ui.enabled=true \
            --set bgpControlPlane.enabled=true \
            --set l2announcements.enabled=true \
            --set loadBalancer.algorithm=maglev \
            --set loadBalancer.mode=snat \
            --set loadBalancer.l2.enabled=false \
            --set loadBalancer.acceleration=disabled \
            --set datapathMode=veth \
            --set enableXDPPrefilter=false \
            --set enable-lb-ipam=true \
            --set operator.replicas=1 \
            --set operator.rollOutPods=true \
            --set k8sServiceHost=172.29.51.10 \
            --set k8sServicePort=6443 \
            --set tolerations[0].key=node-role.kubernetes.io/control-plane \
            --set tolerations[0].operator=Exists \
            --set tolerations[0].effect=NoSchedule \
            --set tolerations[1].key=node-role.kubernetes.io/master \
            --set tolerations[1].operator=Exists \
            --set tolerations[1].effect=NoSchedule \
            --set tolerations[2].key=node.kubernetes.io/not-ready \
            --set tolerations[2].operator=Exists \
            --set tolerations[2].effect=NoSchedule \
            --set tolerations[3].key=node.cilium.io/agent-not-ready \
            --set tolerations[3].operator=Exists \
            --set tolerations[3].effect=NoSchedule
      - echo "Waiting for Cilium to be ready..."
      - sleep 30
      - mise exec -- kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=cilium-agent -n kube-system --timeout=300s || true

  apps:deploy-external-secrets:
    desc: Deploy External Secrets Operator
    cmds:
      - mise exec -- helm repo add external-secrets https://charts.external-secrets.io || true
      - mise exec -- helm repo update
      - mise exec -- helm upgrade --install external-secrets external-secrets/external-secrets --namespace external-secrets-system --create-namespace --wait
      - echo "Waiting for External Secrets CRDs to be ready..."
      - mise exec -- kubectl wait --for condition=established --timeout=120s crd/clustersecretstores.external-secrets.io
      - mise exec -- kubectl wait --for condition=established --timeout=120s crd/secretstores.external-secrets.io
      - echo "Waiting for External Secrets webhook to be fully ready..."
      - mise exec -- kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=external-secrets-webhook -n external-secrets-system --timeout=120s
      - sleep 15

  apps:deploy-onepassword-connect:
    desc: Deploy 1Password Connect
    cmds:
      - echo "Deploying 1Password Connect basic resources..."
      - mise exec -- kubectl apply -f infrastructure/onepassword-connect/namespace.yaml
      - mise exec -- kubectl apply -f infrastructure/onepassword-connect/deployment.yaml
      - mise exec -- kubectl apply -f infrastructure/onepassword-connect/service.yaml
      - echo "Waiting for 1Password Connect to be ready..."
      - mise exec -- kubectl rollout status deployment onepassword-connect -n onepassword-connect --timeout=120s
      - echo "Waiting for External Secrets CRDs to be fully available..."
      - mise exec -- kubectl wait --for condition=established --timeout=60s crd/clustersecretstores.external-secrets.io
      - mise exec -- kubectl wait --for condition=established --timeout=60s crd/secretstores.external-secrets.io
      - echo "Fixing External Secrets webhook if needed..."
      - mise exec -- task apps:fix-external-secrets-webhook
      - echo "Deploying 1Password Connect secret stores..."
      - |
        # Apply secret stores with proper webhook validation
        echo "Applying secret stores..."
        if mise exec -- kubectl apply -f infrastructure/onepassword-connect/secret-store.yaml; then
          echo "‚úì Secret stores deployed successfully"
        else
          echo "‚ö† Secret store deployment failed, fixing webhook and retrying..."
          mise exec -- task apps:fix-external-secrets-webhook
          sleep 10
          mise exec -- kubectl apply -f infrastructure/onepassword-connect/secret-store.yaml
          echo "‚úì Secret stores deployed after webhook fix"
        fi

  apps:fix-external-secrets-webhook:
    desc: Fix External Secrets webhook validation issues
    cmds:
      - echo "Checking External Secrets webhook status..."
      - mise exec -- kubectl get validatingwebhookconfiguration | grep -E "(secretstore|external)" || echo "No webhook configurations found"
      - echo "Reinstalling External Secrets to restore webhook configuration..."
      - mise exec -- helm upgrade --install external-secrets external-secrets/external-secrets --namespace external-secrets-system --reuse-values
      - echo "Waiting for webhook to be ready..."
      - mise exec -- kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=external-secrets-webhook -n external-secrets-system --timeout=120s
      - echo "‚úì External Secrets webhook restored"

  apps:deploy-longhorn:
    desc: Deploy Longhorn storage
    cmds:
      - mise exec -- helm repo add longhorn https://charts.longhorn.io
      - mise exec -- helm repo update
      - mise exec -- helm upgrade --install longhorn longhorn/longhorn -n longhorn-system --create-namespace

  apps:verify-core-idempotency:
    desc: Verify that apps:deploy-core is idempotent and can be run multiple times safely
    cmds:
      - ./scripts/verify-core-idempotency.sh
    preconditions:
      - sh: "mise exec -- kubectl get namespaces &> /dev/null"
        msg: "Kubernetes cluster must be accessible"
      - sh: "[ -x scripts/verify-core-idempotency.sh ]"
        msg: "Idempotency verification script must be executable"

  # BGP Configuration (Multiple Methods)
  bgp:configure-unifi:
    desc: Configure BGP on Unifi UDM Pro (via SSH script - legacy method)
    cmds:
      - echo "Configuring BGP on Unifi UDM Pro via SSH script..."
      - scp scripts/unifi-bgp-config.sh unifi-admin@udm-pro:/tmp/
      - ssh unifi-admin@udm-pro "chmod +x /tmp/unifi-bgp-config.sh && /tmp/unifi-bgp-config.sh"

  bgp:generate-config:
    desc: Generate BGP configuration file for UniFi UI upload (recommended method)
    cmds:
      - echo "BGP configuration file available at scripts/unifi-bgp-config.conf"
      - echo ""
      - echo "To upload this configuration:"
      - echo "1. Open UniFi Network UI"
      - echo "2. Go to Network > Settings > Routing > BGP"
      - echo "3. Click 'Upload Configuration'"
      - echo "4. Select scripts/unifi-bgp-config.conf"
      - echo "5. Apply the configuration"

  bgp:show-config:
    desc: Display BGP configuration file contents
    cmds:
      - echo "=== BGP Configuration for UniFi UDM Pro ==="
      - cat scripts/unifi-bgp-config.conf

  bgp:verify-peering:
    desc: Verify BGP peering status (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP peering status on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp summary'"

  bgp:verify-peering-ipv6:
    desc: Verify BGP IPv6 peering status (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP IPv6 peering status on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast summary'"
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast neighbors'"

  bgp:show-routes-ipv6:
    desc: Show BGP IPv6 routes (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP IPv6 routes on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast'"
      - ssh unifi-admin@udm-pro "vtysh -c 'show ipv6 route bgp'"

  # Network diagnostics
  network:check-lldp:
    desc: Check LLDP neighbors on all nodes
    cmds:
      - echo "Checking LLDP neighbors on {{.NODE_1_IP}}..."
      - mise exec -- talosctl list /sys/class/net --nodes {{.NODE_1_IP}}
      - echo "Checking LLDP neighbors on {{.NODE_2_IP}}..."
      - mise exec -- talosctl list /sys/class/net --nodes {{.NODE_2_IP}}
      - echo "Checking LLDP neighbors on {{.NODE_3_IP}}..."
      - mise exec -- talosctl list /sys/class/net --nodes {{.NODE_3_IP}}

  network:verify-lldpd-config:
    desc: Verify LLDPD configuration and service status (now integrated into main Talos config)
    env:
      TALOSCONFIG: talos/generated/talosconfig
    cmds:
      - echo "=== Verifying LLDPD Configuration and Service Status ==="
      - 'echo "NOTE: LLDPD configuration is now integrated into the main Talos configuration"'
      - echo ""
      - echo "Checking LLDPD service status on all nodes..."
      - |
        mise exec -- talosctl get services --nodes {{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}} | grep lldpd || echo "No LLDPD service found"
      - echo ""
      - echo "Checking for LLDPD configuration file on all nodes..."
      - |
        echo "Node {{.NODE_1_IP}}:"
        mise exec -- talosctl read /etc/lldpd.conf --nodes {{.NODE_1_IP}} 2>/dev/null || echo "  LLDPD config file not found"
        echo "Node {{.NODE_2_IP}}:"
        mise exec -- talosctl read /etc/lldpd.conf --nodes {{.NODE_2_IP}} 2>/dev/null || echo "  LLDPD config file not found"
        echo "Node {{.NODE_3_IP}}:"
        mise exec -- talosctl read /etc/lldpd.conf --nodes {{.NODE_3_IP}} 2>/dev/null || echo "  LLDPD config file not found"
      - echo ""
      - echo "Checking LLDPD environment variables..."
      - |
        echo "Node {{.NODE_1_IP}}:"
        mise exec -- talosctl read /proc/1/environ --nodes {{.NODE_1_IP}} 2>/dev/null | tr '\0' '\n' | grep LLDPD_OPTS || echo "  LLDPD_OPTS not found"
        echo "Node {{.NODE_2_IP}}:"
        mise exec -- talosctl read /proc/1/environ --nodes {{.NODE_2_IP}} 2>/dev/null | tr '\0' '\n' | grep LLDPD_OPTS || echo "  LLDPD_OPTS not found"
        echo "Node {{.NODE_3_IP}}:"
        mise exec -- talosctl read /proc/1/environ --nodes {{.NODE_3_IP}} 2>/dev/null | tr '\0' '\n' | grep LLDPD_OPTS || echo "  LLDPD_OPTS not found"
      - echo ""
      - echo "‚úì LLDPD verification complete"
      - echo "Note - HEALTHY=false is normal for LLDPD (no health checks). RUNNING=true indicates success."

  network:check-usb:
    desc: Check USB device detection on all nodes
    cmds:
      - echo "Checking USB devices on {{.NODE_1_IP}}..."
      - mise exec -- talosctl list /sys/bus/usb/devices --nodes {{.NODE_1_IP}}
      - echo "Checking USB devices on {{.NODE_2_IP}}..."
      - mise exec -- talosctl list /sys/bus/usb/devices --nodes {{.NODE_2_IP}}
      - echo "Checking USB devices on {{.NODE_3_IP}}..."
      - mise exec -- talosctl list /sys/bus/usb/devices --nodes {{.NODE_3_IP}}

  network:check-ipv6:
    desc: Check IPv6 configuration on all nodes
    cmds:
      - echo "Checking IPv6 addresses on {{.NODE_1_IP}}..."
      - mise exec -- talosctl read /proc/net/if_inet6 --nodes {{.NODE_1_IP}}
      - echo "Checking IPv6 addresses on {{.NODE_2_IP}}..."
      - mise exec -- talosctl read /proc/net/if_inet6 --nodes {{.NODE_2_IP}}
      - echo "Checking IPv6 addresses on {{.NODE_3_IP}}..."
      - mise exec -- talosctl read /proc/net/if_inet6 --nodes {{.NODE_3_IP}}

  network:test-ipv6:
    desc: Test IPv6 connectivity from cluster nodes
    cmds:
      - echo "Testing IPv6 connectivity from {{.NODE_1_IP}}..."
      - mise exec -- talosctl read /proc/net/route --nodes {{.NODE_1_IP}}
      - echo "Testing IPv6 connectivity from {{.NODE_2_IP}}..."
      - mise exec -- talosctl read /proc/net/route --nodes {{.NODE_2_IP}}
      - echo "Testing IPv6 connectivity from {{.NODE_3_IP}}..."
      - mise exec -- talosctl read /proc/net/route --nodes {{.NODE_3_IP}}

  # Storage diagnostics
  storage:check-iscsi:
    desc: Check iSCSI configuration on all nodes
    cmds:
      - echo "Checking iSCSI on {{.NODE_1_IP}}..."
      - mise exec -- talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_1_IP}}
      - echo "Checking iSCSI on {{.NODE_2_IP}}..."
      - mise exec -- talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_2_IP}}
      - echo "Checking iSCSI on {{.NODE_3_IP}}..."
      - mise exec -- talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_3_IP}}

  storage:check-longhorn:
    desc: Check Longhorn storage mounts
    cmds:
      - mise exec -- kubectl get pods -n longhorn-system
      - mise exec -- kubectl get nodes -o custom-columns=NAME:.metadata.name,LONGHORN-READY:.status.conditions[?(@.type=="LonghornReady")].status

  storage:check-usb-ssd:
    desc: Check USB SSD detection and mounting on all nodes
    cmds:
      - echo "Checking USB SSD detection on {{.NODE_1_IP}}..."
      - mise exec -- talosctl get disks --nodes {{.NODE_1_IP}}
      - mise exec -- talosctl ls /dev/disk/by-id/ --nodes {{.NODE_1_IP}} | grep usb || echo "No USB devices found"
      - echo "Checking USB SSD detection on {{.NODE_2_IP}}..."
      - mise exec -- talosctl get disks --nodes {{.NODE_2_IP}}
      - mise exec -- talosctl ls /dev/disk/by-id/ --nodes {{.NODE_2_IP}} | grep usb || echo "No USB devices found"
      - echo "Checking USB SSD detection on {{.NODE_3_IP}}..."
      - mise exec -- talosctl get disks --nodes {{.NODE_3_IP}}
      - mise exec -- talosctl ls /dev/disk/by-id/ --nodes {{.NODE_3_IP}} | grep usb || echo "No USB devices found"

  storage:validate-usb-ssd:
    desc: Validate USB SSD storage configuration
    cmds:
      - ./scripts/validate-usb-ssd-storage.sh
    preconditions:
      - sh: "[ -x scripts/validate-usb-ssd-storage.sh ]"
        msg: "USB SSD validation script must be executable"

  storage:deploy-usb-ssd:
    desc: Deploy USB SSD storage configuration
    cmds:
      - ./scripts/deploy-usb-ssd-storage.sh
    preconditions:
      - sh: "[ -x scripts/deploy-usb-ssd-storage.sh ]"
        msg: "USB SSD deployment script must be executable"

  storage:validate-complete-usb-ssd:
    desc: Run comprehensive USB SSD validation
    cmds:
      - ./scripts/validate-complete-usb-ssd-setup.sh
    preconditions:
      - sh: "[ -x scripts/validate-complete-usb-ssd-setup.sh ]"
        msg: "Complete USB SSD validation script must be executable"

  storage:validate-longhorn-usb-ssd:
    desc: Validate Longhorn USB SSD integration
    cmds:
      - ./scripts/validate-longhorn-usb-ssd.sh
    preconditions:
      - sh: "[ -x scripts/validate-longhorn-usb-ssd.sh ]"
        msg: "Longhorn USB SSD validation script must be executable"

  # Testing tasks
  test:all:
    desc: Run all tests
    cmds:
      - mise exec -- task test:config
      - mise exec -- task test:connectivity
      - mise exec -- task test:extensions
      - mise exec -- task test:usb-storage
      - mise exec -- task test:recovery

  test:config:
    desc: Test configuration validity
    cmds:
      - echo "Testing Talos configuration..."
      - mise exec -- talosctl validate --config talos/generated/controlplane.yaml
      - echo "Testing Kubernetes manifests..."
      - mise exec -- kubectl apply --dry-run=client -f clusters/home-ops/

  test:connectivity:
    desc: Test cluster connectivity
    cmds:
      - echo "Testing cluster connectivity..."
      - mise exec -- kubectl get nodes
      - mise exec -- kubectl get pods --all-namespaces

  test:extensions:
    desc: Test Talos extensions including LLDPD configuration
    cmds:
      - echo "Testing Talos extensions..."
      - mise exec -- task talos:check-extensions
      - mise exec -- task network:verify-lldpd-config
      - mise exec -- task network:check-usb
      - mise exec -- task storage:check-iscsi
      - mise exec -- task storage:validate-usb-ssd

  test:usb-storage:
    desc: Test USB SSD storage configuration and functionality
    cmds:
      - echo "Testing USB SSD storage configuration..."
      - ./scripts/validate-complete-usb-ssd-setup.sh --quick
      - mise exec -- task storage:check-usb-ssd
    preconditions:
      - sh: "[ -x scripts/validate-complete-usb-ssd-setup.sh ]"
        msg: "USB SSD validation script must be executable"

  test:ipv6:
    desc: Test IPv6 configuration and connectivity
    cmds:
      - echo "Testing IPv6 configuration..."
      - mise exec -- task network:check-ipv6
      - mise exec -- task bgp:verify-peering-ipv6
      - mise exec -- kubectl get svc -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,IPv4:.status.loadBalancer.ingress[*].ip,IPv6:.status.loadBalancer.ingress[*].ip

  test:recovery:
    desc: Test cluster recovery functionality
    cmds:
      - echo "Testing cluster recovery functionality..."
      - ./tests/test-recovery.sh

  # Maintenance tasks
  maintenance:backup:
    desc: Backup cluster configuration
    cmds:
      - mkdir -p backups/$(date +%Y%m%d-%H%M%S)
      - mise exec -- kubectl get all --all-namespaces -o yaml > backups/$(date +%Y%m%d-%H%M%S)/cluster-state.yaml
      - mise exec -- talosctl get machineconfig -o yaml > backups/$(date +%Y%m%d-%H%M%S)/talos-config.yaml

  maintenance:cleanup:
    desc: Cleanup old resources
    cmds:
      - mise exec -- kubectl delete pods --field-selector=status.phase=Succeeded --all-namespaces
      - mise exec -- kubectl delete pods --field-selector=status.phase=Failed --all-namespaces

  # Development tasks
  dev:port-forward:
    desc: Port forward common services
    cmds:
      - mise exec -- kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80 &
      - mise exec -- kubectl port-forward -n onepassword-connect svc/onepassword-connect 8081:8080 &
      - echo "Services available at localhost:8080 (Longhorn) and localhost:8081 (1Password Connect)"

  dev:logs:
    desc: Tail logs for debugging
    cmds:
      - mise exec -- kubectl logs -f -n kube-system -l app.kubernetes.io/name=cilium

  # Cluster status
  cluster:status:
    desc: Show comprehensive cluster status for all-control-plane cluster
    cmds:
      - echo "=== All-Control-Plane Cluster Nodes ==="
      - mise exec -- kubectl get nodes -o wide
      - echo ""
      - echo "=== Node Roles (should show 'control-plane' for all nodes) ==="
      - mise exec -- kubectl get nodes --show-labels | grep "node-role.kubernetes.io"
      - echo ""
      - echo "=== etcd Members ==="
      - mise exec -- kubectl get pods -n kube-system -l component=etcd -o wide
      - echo ""
      - echo "=== Control Plane Components ==="
      - mise exec -- kubectl get pods -n kube-system -l tier=control-plane -o wide
      - echo ""
      - echo "=== System Pods ==="
      - mise exec -- kubectl get pods -n kube-system
      - echo ""
      - echo "=== Longhorn Status ==="
      - mise exec -- kubectl get pods -n longhorn-system
      - echo ""
      - echo "=== BGP Status ==="
      - mise exec -- kubectl get ciliumbgpclusterconfig
      - echo ""
      - echo "=== LoadBalancer Services ==="
      - mise exec -- kubectl get svc --all-namespaces | grep LoadBalancer

  # Renovate tasks
  renovate:dry-run:
    desc: Run Renovate in dry-run mode to see what updates are available
    cmds:
      - echo "Running Renovate dry-run to check for available updates..."
      - RENOVATE_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") npx renovate --dry-run geoffdavis/talos-gitops
    preconditions:
      - sh: "command -v op"
        msg: "1Password CLI (op) is required to retrieve GitHub token"

  renovate:run:
    desc: Run Renovate locally (creates actual PRs)
    cmds:
      - echo "Running Renovate to create update PRs..."
      - echo "WARNING - This will create actual PRs if updates are found"
      - echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
      - sleep 5
      - RENOVATE_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") npx renovate geoffdavis/talos-gitops
    preconditions:
      - sh: "command -v op"
        msg: "1Password CLI (op) is required to retrieve GitHub token"

  renovate:validate:
    desc: Validate Renovate configuration
    cmds:
      - echo "Validating Renovate configuration..."
      - npx renovate-config-validator .renovaterc.json

  renovate:install:
    desc: Install Renovate CLI globally
    cmds:
      - echo "Installing Renovate CLI..."
      - npm install -g renovate

  # 1Password Connect automation
  onepassword:create-connect-server:
    desc: Create new 1Password Connect server with proper credentials and store in 1Password
    preconditions:
      - sh: "command -v op"
        msg: "1Password CLI (op) is required"
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: "OP_ACCOUNT environment variable must be set"
    cmds:
      - echo "Creating new 1Password Connect server with credentials..."
      - |
        # Create Connect server with proper vault access
        echo "Creating Connect server for home-ops cluster..."
        op connect server create "home-ops-cluster" --vaults "Automation,Services" > /dev/null

        # Verify credentials file was created
        if [ ! -f "1password-credentials.json" ]; then
          echo "‚ùå Credentials file not created"
          exit 1
        fi

        # Validate JSON format
        if ! jq . 1password-credentials.json > /dev/null; then
          echo "‚ùå Invalid credentials JSON format"
          exit 1
        fi

        echo "‚úì Valid credentials file created ($(wc -c < 1password-credentials.json) bytes)"
      - |
        # Store credentials as document in 1Password
        echo "Storing credentials in 1Password as document..."
        op document create 1password-credentials.json \
          --title="1Password Connect Credentials - home-ops" \
          --vault="Automation" > /dev/null
        echo "‚úì Credentials stored as document in 1Password"
      - |
        # Create Connect token
        echo "Creating Connect token..."
        CONNECT_TOKEN=$(op connect token create "home-ops-token" \
          --server "home-ops-cluster" \
          --vault "Automation" \
          --expires-in 8760h)

        if [ ${#CONNECT_TOKEN} -lt 100 ]; then
          echo "‚ùå Connect token appears invalid (length: ${#CONNECT_TOKEN})"
          exit 1
        fi

        echo "‚úì Connect token created (length: ${#CONNECT_TOKEN})"

        # Store token in 1Password
        echo "Storing Connect token in 1Password..."
        op item create \
          --category="API Credential" \
          --title="1Password Connect Token - home-ops" \
          --vault="Automation" \
          "token[password]=$CONNECT_TOKEN" > /dev/null || \
        op item edit "1Password Connect Token - home-ops" \
          "token[password]=$CONNECT_TOKEN" \
          --vault="Automation" > /dev/null
        echo "‚úì Connect token stored in 1Password"
      - |
        # Update Kubernetes secrets
        echo "Updating Kubernetes secrets..."
        mise exec -- kubectl create secret generic onepassword-connect-credentials \
          --from-file=1password-credentials.json \
          --namespace=onepassword-connect \
          --dry-run=client -o yaml | mise exec -- kubectl apply -f -

        CONNECT_TOKEN=$(op connect token create "home-ops-token" \
          --server "home-ops-cluster" \
          --vault "Automation" \
          --expires-in 8760h)

        mise exec -- kubectl create secret generic onepassword-connect-token \
          --from-literal=token="$CONNECT_TOKEN" \
          --namespace=onepassword-connect \
          --dry-run=client -o yaml | mise exec -- kubectl apply -f -

        echo "‚úì Kubernetes secrets updated"
      - |
        # Restart 1Password Connect deployment
        echo "Restarting 1Password Connect deployment..."
        mise exec -- kubectl rollout restart deployment onepassword-connect -n onepassword-connect
        mise exec -- kubectl rollout status deployment onepassword-connect -n onepassword-connect --timeout=120s
        echo "‚úì 1Password Connect deployment restarted"
      - |
        # Clean up local files
        echo "Cleaning up local files..."
        rm -f 1password-credentials.json connect-token.txt
        echo "‚úì Local files cleaned up"
      - echo "‚úÖ 1Password Connect server setup completed successfully!"

  onepassword:fix-credentials:
    desc: Fix 1Password Connect credentials truncation issue
    deps: [onepassword:create-connect-server]
    cmds:
      - echo "1Password Connect credentials have been regenerated and updated"
      - mise exec -- task bootstrap:validate-1password-secrets

  # Cluster Safety Tasks - CRITICAL SAFETY MEASURES
  cluster:safe-reset:
    desc: SAFE cluster reset - only resets EPHEMERAL and STATE partitions (preserves OS)
    env:
      TALOSCONFIG: talos/generated/talosconfig
    vars:
      CONFIRM: '{{.CONFIRM | default ""}}'
      NODES: '{{.NODES | default "all"}}'
    cmds:
      - echo "‚ö†Ô∏è  CLUSTER SAFE RESET - This will reset EPHEMERAL and STATE partitions only"
      - echo "‚ö†Ô∏è  This operation will:"
      - echo "   - Reset user data and configurations"
      - echo "   - Preserve the Talos OS installation"
      - echo "   - Require cluster re-bootstrap after reset"
      - echo ""
      - echo "‚ùå NEVER use 'talosctl reset' without partition specifications!"
      - echo "‚ùå That command WILL WIPE THE ENTIRE OS and require USB reinstallation!"
      - echo ""
      - |
        if [ "{{.CONFIRM}}" != "SAFE-RESET" ]; then
          echo "‚ùå Safety confirmation required. Run with: task cluster:safe-reset CONFIRM=SAFE-RESET"
          echo "   Optional: Specify nodes with NODES=node1,node2 (default: all nodes)"
          exit 1
        fi
      - |
        # Determine which nodes to reset
        if [ "{{.NODES}}" = "all" ]; then
          RESET_NODES="{{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}}"
          echo "Performing SAFE reset on ALL nodes (EPHEMERAL and STATE partitions only)..."
        else
          RESET_NODES="{{.NODES}}"
          echo "Performing SAFE reset on specified nodes: {{.NODES}} (EPHEMERAL and STATE partitions only)..."
        fi

        echo "Target nodes: $RESET_NODES"

        # Store original cluster state for validation
        echo "Capturing current cluster state for validation..."
        ORIGINAL_CLUSTER_ID=""
        if mise exec -- kubectl get nodes >/dev/null 2>&1; then
          ORIGINAL_CLUSTER_ID=$(mise exec -- kubectl get nodes -o jsonpath='{.items[0].metadata.uid}' 2>/dev/null || echo "")
          echo "Original cluster detected with ID: ${ORIGINAL_CLUSTER_ID:0:8}..."
        else
          echo "No accessible cluster detected - proceeding with reset"
        fi

        # Perform the reset with proper error handling
        echo "Attempting reset with partition specifications..."
        RESET_SUCCESS=false

        # Try simultaneous reset first (with --wait=false for authenticated mode)
        echo "Attempting simultaneous reset..."
        if mise exec -- talosctl reset --nodes $RESET_NODES --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false; then
          echo "‚úÖ Simultaneous reset command executed successfully"
          RESET_SUCCESS=true
        else
          echo "Simultaneous reset failed, trying individual resets..."

          # Parse nodes and reset individually
          NODES_LIST=$(echo "$RESET_NODES" | tr ',' ' ')
          INDIVIDUAL_SUCCESS=0
          TOTAL_NODES=$(echo $NODES_LIST | wc -w)

          for node in $NODES_LIST; do
            echo "Resetting node $node (SAFE - partitions only)..."

            # Try with talosconfig first (with --wait=false for authenticated mode)
            if mise exec -- talosctl reset --nodes $node --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false; then
              echo "‚úÖ Reset command sent to $node (authenticated)"
              INDIVIDUAL_SUCCESS=$((INDIVIDUAL_SUCCESS + 1))
            else
              echo "‚ö† Authenticated reset failed for $node, trying insecure mode..."
              # Try insecure mode for nodes with certificate issues (must explicitly disable --wait)
              if mise exec -- talosctl reset --insecure --nodes $node --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false --reboot; then
                echo "‚úÖ Reset command sent to $node (insecure mode)"
                INDIVIDUAL_SUCCESS=$((INDIVIDUAL_SUCCESS + 1))
              else
                echo "‚ùå Failed to reset $node in both authenticated and insecure modes"
                # Show the actual error for debugging
                echo "Debug: Attempting reset with error output..."
                mise exec -- talosctl reset --insecure --nodes $node --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false --reboot 2>&1 || true
              fi
            fi
            sleep 5
          done

          if [ $INDIVIDUAL_SUCCESS -gt 0 ]; then
            echo "‚úÖ Reset commands sent to $INDIVIDUAL_SUCCESS/$TOTAL_NODES nodes"
            RESET_SUCCESS=true
          fi
        fi

        if [ "$RESET_SUCCESS" = "false" ]; then
          echo "‚ùå Failed to send reset commands to any nodes"
          echo "This may indicate:"
          echo "  - Nodes are already in maintenance mode"
          echo "  - Network connectivity issues"
          echo "  - Certificate/authentication problems"
          echo ""
          echo "Manual verification required:"
          echo "  1. Check if nodes are accessible: ping {{.NODE_1_IP}}"
          echo "  2. Try insecure mode: talosctl reset --insecure --nodes {{.NODE_1_IP}} --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --reboot"
          exit 1
        fi
      - |
        # Wait for nodes to reboot and validate reset
        echo ""
        echo "üîÑ Waiting for nodes to reboot and validate reset completion..."
        echo "This may take 2-3 minutes..."

        # Wait for nodes to go down first
        echo "Waiting for nodes to go offline..."
        sleep 30

        # Wait for nodes to come back up
        echo "Waiting for nodes to come back online..."
        max_wait=300  # 5 minutes
        wait_interval=15
        elapsed=0
        nodes_online=0

        NODES_LIST="{{.NODE_1_IP}} {{.NODE_2_IP}} {{.NODE_3_IP}}"

        while [ $elapsed -lt $max_wait ]; do
          nodes_online=0

          for node_ip in $NODES_LIST; do
            if ping -c 1 -W 2 "$node_ip" >/dev/null 2>&1; then
              nodes_online=$((nodes_online + 1))
            fi
          done

          echo "Nodes online: $nodes_online/3 (${elapsed}s elapsed)"

          if [ $nodes_online -eq 3 ]; then
            echo "‚úÖ All nodes are back online"
            break
          fi

          sleep $wait_interval
          elapsed=$((elapsed + wait_interval))
        done

        if [ $nodes_online -lt 3 ]; then
          echo "‚ö† Only $nodes_online/3 nodes came back online after ${max_wait}s"
          echo "This may be normal - nodes might need more time to fully boot"
        fi
      - |
        # Validate that reset was successful
        echo ""
        echo "üîç Validating reset completion..."

        # Wait a bit more for Talos to fully initialize
        echo "Waiting for Talos to initialize..."
        sleep 45

        # Check if nodes are in maintenance mode (expected after reset)
        NODES_IN_MAINTENANCE=0
        NODES_LIST="{{.NODE_1_IP}} {{.NODE_2_IP}} {{.NODE_3_IP}}"

        for node_ip in $NODES_LIST; do
          echo "Checking node $node_ip..."

          # Try to connect with insecure mode (expected for maintenance mode)
          if mise exec -- talosctl version --insecure --nodes $node_ip >/dev/null 2>&1; then
            echo "  ‚úÖ Node $node_ip is accessible (maintenance mode)"
            NODES_IN_MAINTENANCE=$((NODES_IN_MAINTENANCE + 1))

            # Check if it's actually in maintenance mode (no cluster config)
            if ! mise exec -- talosctl get machineconfig --insecure --nodes $node_ip >/dev/null 2>&1; then
              echo "  ‚úÖ Node $node_ip confirmed in maintenance mode (no cluster config)"
            else
              echo "  ‚ö† Node $node_ip has cluster config - reset may not have completed"
            fi
          else
            echo "  ‚ö† Node $node_ip not accessible yet"
          fi
        done

        echo ""
        if [ $NODES_IN_MAINTENANCE -eq 3 ]; then
          echo "‚úÖ RESET VALIDATION SUCCESSFUL"
          echo "   - All 3 nodes are accessible in maintenance mode"
          echo "   - Nodes are ready for fresh cluster bootstrap"
          echo "   - OS preserved, only user data/config wiped"
        elif [ $NODES_IN_MAINTENANCE -gt 0 ]; then
          echo "‚ö† PARTIAL RESET SUCCESS"
          echo "   - $NODES_IN_MAINTENANCE/3 nodes confirmed in maintenance mode"
          echo "   - Some nodes may need additional time or manual intervention"
        else
          echo "‚ùå RESET VALIDATION FAILED"
          echo "   - No nodes confirmed in maintenance mode"
          echo "   - Reset may not have completed successfully"
          echo "   - Manual verification required"
          exit 1
        fi
      - echo ""
      - echo "‚úÖ SAFE reset completed and validated. OS preserved."
      - echo "üìã Next steps:"
      - echo "   1. Run 'mise exec -- task bootstrap:phase-1' to start fresh bootstrap"
      - echo "   2. Or run 'mise exec -- task bootstrap:phased' for complete rebuild"
      - echo ""
      - |
        echo "üîç To verify maintenance mode: mise exec -- talosctl version --insecure --nodes {{.NODE_1_IP}}"

  cluster:emergency-recovery:
    desc: Emergency cluster recovery with comprehensive safety checks
    env:
      TALOSCONFIG: talos/generated/talosconfig
    cmds:
      - echo "üö® EMERGENCY CLUSTER RECOVERY PROCEDURE"
      - echo "This will attempt to recover the cluster using safe methods only"
      - echo ""
      - |
        read -p "Confirm emergency recovery (y/N): " confirm
        [ "$confirm" = "y" ]
      - echo "Step 1 - Verifying cluster safety..."
      - mise exec -- task cluster:verify-safety
      - echo "Step 2 - Attempting kubeconfig recovery..."
      - mise exec -- task talos:recover-kubeconfig
      - echo "Step 3 - Checking node status..."
      - mise exec -- kubectl get nodes || echo "Nodes not accessible - may need safe reset"
      - echo "Step 4 - Checking critical pods..."
      - mise exec -- kubectl get pods -n kube-system || echo "Control plane not accessible"
      - echo "Step 5 - If cluster is unrecoverable, use mise exec -- task cluster:safe-reset"
      - echo "‚úÖ Emergency recovery assessment complete"

  cluster:safe-reboot:
    desc: Safe cluster reboot (alternative to reset operations)
    env:
      TALOSCONFIG: talos/generated/talosconfig
    cmds:
      - echo "üîÑ SAFE CLUSTER REBOOT"
      - echo "This will reboot all nodes safely without data loss"
      - echo ""
      - |
        read -p "Confirm safe reboot of all nodes (y/N): " confirm
        [ "$confirm" = "y" ]
      - echo "Rebooting all nodes safely..."
      - |
        echo "Rebooting {{.NODE_1_IP}}..."
        mise exec -- talosctl reboot --nodes {{.NODE_1_IP}}
        sleep 45
      - |
        echo "Rebooting {{.NODE_2_IP}}..."
        mise exec -- talosctl reboot --nodes {{.NODE_2_IP}}
        sleep 45
      - |
        echo "Rebooting {{.NODE_3_IP}}..."
        mise exec -- talosctl reboot --nodes {{.NODE_3_IP}}
        sleep 45
      - echo "Waiting for cluster to come back online..."
      - sleep 60
      - mise exec -- task talos:recover-kubeconfig
      - mise exec -- kubectl get nodes
      - echo "‚úÖ Safe reboot completed"

  cluster:verify-safety:
    desc: Verify cluster safety and warn about dangerous operations
    cmds:
      - echo "üîç CLUSTER SAFETY VERIFICATION"
      - echo ""
      - echo "‚úÖ SAFE OPERATIONS:"
      - echo "   - talosctl reset --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL"
      - echo "   - talosctl reboot"
      - echo "   - talosctl upgrade"
      - echo "   - task cluster:safe-reset"
      - echo "   - task cluster:safe-reboot"
      - echo ""
      - echo "‚ùå DANGEROUS OPERATIONS (NEVER USE):"
      - echo "   - talosctl reset (without partition specifications)"
      - echo "   - Any reset command that doesn't specify partitions"
      - echo ""
      - echo "üìã CURRENT CLUSTER STATUS:"
      - mise exec -- kubectl get nodes -o wide || echo "Cluster not accessible"
      - echo ""
      - echo "üìñ For detailed safety information, see:"
      - echo "   - docs/CLUSTER_RESET_SAFETY.md"
      - echo "   - docs/SUBTASK_SAFETY_GUIDELINES.md"

# Include modular taskfiles for phased bootstrap system
includes:
  bootstrap: ./taskfiles/bootstrap.yml
  validation: ./taskfiles/validation.yml
  talos: ./taskfiles/talos.yml
  networking: ./taskfiles/networking.yml
  services: ./taskfiles/services.yml
  gitops: ./taskfiles/gitops.yml
  applications: ./taskfiles/applications.yml
  bgp-loadbalancer: ./taskfiles/bgp-loadbalancer.yml
  pre-commit: ./taskfiles/pre-commit.yml
