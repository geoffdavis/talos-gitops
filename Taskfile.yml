version: '3'

dotenv: ['.env']

vars:
  CLUSTER_NAME: home-ops
  CONTROL_PLANE_SUBNET: 172.29.51.0/24
  # renovate: datasource=github-releases depName=siderolabs/talos
  TALOS_VERSION: v1.10.5
  # renovate: datasource=github-releases depName=kubernetes/kubernetes
  KUBERNETES_VERSION: v1.31.1
  CLUSTER_ENDPOINT: https://172.29.51.10:6443
  NODE_1_IP: 172.29.51.11
  NODE_2_IP: 172.29.51.12
  NODE_3_IP: 172.29.51.13
  TALOSCONFIG: talos/generated/talosconfig

tasks:
  # Bootstrap tasks
  bootstrap:secrets:
    desc: Bootstrap secrets from 1Password (requires OP_ACCOUNT environment variable)
    cmds:
      - ./scripts/bootstrap-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'

  bootstrap:k8s-secrets:
    desc: Bootstrap Kubernetes secrets from 1Password (for existing cluster)
    cmds:
      - ./scripts/bootstrap-k8s-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'
      - sh: 'mise exec -- kubectl get namespaces &> /dev/null'
        msg: 'Kubernetes cluster must be accessible'

  bootstrap:1password-secrets:
    desc: Bootstrap 1Password Connect secrets for fresh cluster (requires OP_ACCOUNT environment variable)
    cmds:
      - ./scripts/bootstrap-1password-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'

  bootstrap:validate-1password-secrets:
    desc: Validate 1Password Connect secrets were created correctly
    cmds:
      - ./scripts/validate-1password-secrets.sh

  bootstrap:cluster:
    desc: Bootstrap the entire cluster from scratch with LLDPD stability fix
    deps: [bootstrap:secrets, talos:generate-config]
    cmds:
      - mise exec -- task talos:apply-config
      - mise exec -- task talos:apply-lldpd-config
      - mise exec -- task talos:bootstrap
      - mise exec -- task bootstrap:1password-secrets
      - mise exec -- task bootstrap:validate-1password-secrets
      - mise exec -- task flux:bootstrap
      - mise exec -- task apps:deploy-core

  # Talos tasks
  talos:generate-schematic:
    desc: Generate custom Talos schematic with extensions via Image Factory
    cmds:
      - ./scripts/generate-talos-schematic.sh
    preconditions:
      - sh: 'command -v curl'
        msg: 'curl is required for Image Factory API'
      - sh: 'command -v jq'
        msg: 'jq is required for JSON processing'

  talos:update-installer-images:
    desc: Update machine configurations to use custom installer images
    deps: [talos:generate-schematic]
    cmds:
      - ./scripts/update-installer-images.sh
    preconditions:
      - sh: '[ -f talos/generated/schematic-id.txt ]'
        msg: 'Schematic ID file not found. Run task talos:generate-schematic first.'

  talos:restore-secrets:
    desc: Restore Talos secrets from the correct 1Password entry (talos - home-ops)
    preconditions:
      - sh: '[ -n "${OP_ACCOUNT:-}" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'
    cmds:
      - echo "Restoring Talos secrets from 1Password 'talos - {{.CLUSTER_NAME}}' entry..."
      - |
        op --account {{.OP_ACCOUNT}} item get "talos - {{.CLUSTER_NAME}}" --vault="Automation" --format json | jq -r '{
          cluster: {
            id: .fields[] | select(.label == "cluster_id") | .value,
            secret: .fields[] | select(.label == "cluster_secret") | .value
          },
          secrets: {
            bootstraptoken: .fields[] | select(.label == "bootstraptoken") | .value,
            secretboxencryptionsecret: .fields[] | select(.label == "secretboxencryptionsecret") | .value
          },
          trustdinfo: {
            token: .fields[] | select(.label == "trustdinfo_token") | .value
          },
          certs: {
            etcd: {
              crt: .fields[] | select(.label == "cert_etcd_crt") | .value,
              key: .fields[] | select(.label == "cert_etcd_key") | .value
            },
            k8s: {
              crt: .fields[] | select(.label == "cert_k8s_crt") | .value,
              key: .fields[] | select(.label == "cert_k8s_key") | .value
            },
            k8saggregator: {
              crt: .fields[] | select(.label == "cert_k8saggregator_crt") | .value,
              key: .fields[] | select(.label == "cert_k8saggregator_key") | .value
            },
            k8sserviceaccount: {
              key: .fields[] | select(.label == "cert_k8sserviceaccount_key") | .value
            },
            os: {
              crt: .fields[] | select(.label == "cert_os_crt") | .value,
              key: .fields[] | select(.label == "cert_os_key") | .value
            }
          }
        }' | yq -P > talos/talsecret.yaml
      - echo "Secrets restored from 1Password"

  talos:generate-config:
    desc: Generate Talos configuration using talhelper with 1Password integration
    preconditions:
      - sh: 'mise exec -- talhelper --version'
        msg: 'talhelper is required and managed by mise'
      - sh: '[ -n "${OP_ACCOUNT:-}" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'
    cmds:
      - echo "Generating Talos configuration with talhelper and 1Password integration..."
      - mkdir -p talos/generated
      - |
        # Check for existing secrets in multiple locations
        # First check for "talos - home-ops" (legacy entry)
        if op --account {{.OP_ACCOUNT}} item get "talos - {{.CLUSTER_NAME}}" --vault="Automation" &> /dev/null; then
          echo "Found legacy secrets entry 'talos - {{.CLUSTER_NAME}}', restoring..."
          mise exec -- task talos:restore-secrets
        # Then check for "Talos Secrets - home-ops" (new format)
        elif op item get "Talos Secrets - {{.CLUSTER_NAME}}" &> /dev/null; then
          echo "Retrieving existing secrets from 1Password..."
          op item get "Talos Secrets - {{.CLUSTER_NAME}}" --fields label=talsecret --format json | jq -r '.value' > talos/talsecret.yaml
          echo "Using existing secrets from 1Password"
        elif [[ -f talos/talsecret.yaml ]]; then
          echo "Using existing local secrets file..."
        else
          echo "Generating new Talos secrets..."
          mise exec -- talhelper gensecret > talos/talsecret.yaml
          
          # Store new secrets in 1Password
          echo "Storing new secrets in 1Password..."
          if ! op item get "Talos Secrets - {{.CLUSTER_NAME}}" &> /dev/null; then
            op item create \
              --category="Secure Note" \
              --title="Talos Secrets - {{.CLUSTER_NAME}}" \
              --vault="Automation" \
              "talsecret[password]=$(cat talos/talsecret.yaml)"
          else
            op item edit "Talos Secrets - {{.CLUSTER_NAME}}" \
              "talsecret[password]=$(cat talos/talsecret.yaml)"
          fi
          echo "Secrets stored in 1Password"
        fi
      - |
        # Generate configuration files
        echo "Generating Talos configuration files..."
        mise exec -- talhelper genconfig --secret-file talos/talsecret.yaml
      - echo "Generated Talos configuration with talhelper"
      - echo "Configuration files created in clusterconfig/"

  talos:apply-config:
    desc: Apply Talos configuration to nodes
    deps: [talos:generate-config]
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Applying Talos configuration to all control plane nodes..."
      - echo "NOTE - Certificate errors are expected for nodes in maintenance mode - will retry with insecure mode"
      - |
        echo "Applying configuration to mini01 ({{.NODE_1_IP}})..."
        if mise exec -- talosctl apply-config --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml 2>/dev/null; then
          echo "✓ mini01 configuration applied successfully"
        else
          echo "⚠ Certificate error expected for maintenance mode - retrying with insecure mode..."
          if mise exec -- talosctl apply-config --insecure --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml; then
            echo "✓ mini01 configuration applied successfully (insecure mode - normal for maintenance)"
          else
            echo "✗ Failed to apply configuration to mini01"
            exit 1
          fi
        fi
      - |
        echo "Applying configuration to mini02 ({{.NODE_2_IP}})..."
        if mise exec -- talosctl apply-config --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml 2>/dev/null; then
          echo "✓ mini02 configuration applied successfully"
        else
          echo "⚠ Certificate error expected for maintenance mode - retrying with insecure mode..."
          if mise exec -- talosctl apply-config --insecure --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml; then
            echo "✓ mini02 configuration applied successfully (insecure mode - normal for maintenance)"
          else
            echo "✗ Failed to apply configuration to mini02"
            exit 1
          fi
        fi
      - |
        echo "Applying configuration to mini03 ({{.NODE_3_IP}})..."
        if mise exec -- talosctl apply-config --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml 2>/dev/null; then
          echo "✓ mini03 configuration applied successfully"
        else
          echo "⚠ Certificate error expected for maintenance mode - retrying with insecure mode..."
          if mise exec -- talosctl apply-config --insecure --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml; then
            echo "✓ mini03 configuration applied successfully (insecure mode - normal for maintenance)"
          else
            echo "✗ Failed to apply configuration to mini03 at {{.NODE_3_IP}}"
            echo "Checking if node is reachable..."
            if mise exec -- talosctl version --nodes {{.NODE_3_IP}} --timeout 10s 2>/dev/null; then
              echo "Node is reachable but configuration failed"
            else
              echo "Node is not reachable - check network connectivity"
            fi
            exit 1
          fi
        fi
      - echo "✓ Configuration successfully applied to all nodes"

  talos:apply-config-only:
    desc: Apply Talos configuration to nodes (without regenerating)
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Applying Talos configuration to all control plane nodes..."
      - |
        # Try with talosconfig first, fall back to insecure if needed
        mise exec -- talosctl apply-config --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml || \
        mise exec -- talosctl apply-config --insecure --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml
      - |
        mise exec -- talosctl apply-config --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml || \
        mise exec -- talosctl apply-config --insecure --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml
      - |
        mise exec -- talosctl apply-config --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml || \
        mise exec -- talosctl apply-config --insecure --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml
      - echo "Configuration applied to all nodes"

  talos:apply-lldpd-config:
    desc: Apply LLDPD ExtensionServiceConfig to all nodes for stability
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    preconditions:
      - sh: '[ -f talos/manifests/lldpd-extension-config.yaml ]'
        msg: 'LLDPD ExtensionServiceConfig manifest not found at talos/manifests/lldpd-extension-config.yaml'
    cmds:
      - echo "Applying LLDPD ExtensionServiceConfig to all nodes..."
      - echo "This prevents LLDPD service startup failures that cause periodic reboots"
      - |
        mise exec -- talosctl patch machineconfig \
          --nodes {{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}} \
          --patch-file talos/manifests/lldpd-extension-config.yaml
      - echo "Waiting for LLDPD configuration to be applied..."
      - sleep 15
      - echo "Verifying LLDPD ExtensionServiceConfig is loaded..."
      - |
        echo "Checking ExtensionServiceConfig on all nodes..."
        mise exec -- talosctl get extensionserviceconfigs --nodes {{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}} || echo "ExtensionServiceConfigs will be visible after next reboot"
      - echo "✓ LLDPD configuration applied successfully to all nodes"
      - echo "✓ This prevents the periodic reboot issues caused by LLDPD service failures"

  talos:bootstrap:
    desc: Bootstrap Talos cluster with all control plane nodes
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Bootstrapping first control plane node..."
      - mise exec -- talosctl bootstrap --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}}
      - echo "Waiting for cluster to initialize..."
      - sleep 30
      - echo "Getting kubeconfig from cluster..."
      - mise exec -- talosctl kubeconfig --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}} --force
      - echo "All three nodes are now functioning as control plane nodes"

  talos:convert-to-all-controlplane:
    desc: Convert existing cluster to all-control-plane setup
    deps: [talos:generate-config]
    cmds:
      - echo "Converting cluster to all-control-plane setup..."
      - echo "Applying control plane configuration to all nodes..."
      - mise exec -- talosctl apply-config --nodes {{.NODE_1_IP}} --file talos/generated/controlplane.yaml
      - mise exec -- talosctl apply-config --nodes {{.NODE_2_IP}} --file talos/generated/controlplane.yaml
      - mise exec -- talosctl apply-config --nodes {{.NODE_3_IP}} --file talos/generated/controlplane.yaml
      - echo "Waiting for nodes to restart and join as control planes..."
      - sleep 60
      - echo "Verifying cluster status..."
      - mise exec -- kubectl get nodes -o wide
      - echo "All nodes should now show as control planes with 'control-plane' role"

  talos:upgrade:
    desc: Upgrade Talos nodes
    cmds:
      - mise exec -- talosctl upgrade --nodes {{.NODE_1_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}
      - mise exec -- talosctl upgrade --nodes {{.NODE_2_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}
      - mise exec -- talosctl upgrade --nodes {{.NODE_3_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}

  talos:reboot:
    desc: Reboot specified nodes or all nodes (for USB detection)
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    vars:
      NODES: '{{.NODES | default "all"}}'
    cmds:
      - |
        if [ "{{.NODES}}" = "all" ]; then
          echo "Rebooting all nodes for USB device detection..."
          mise exec -- talosctl reboot --nodes {{.NODE_1_IP}}
          sleep 30
          mise exec -- talosctl reboot --nodes {{.NODE_2_IP}}
          sleep 30
          mise exec -- talosctl reboot --nodes {{.NODE_3_IP}}
          echo "All nodes rebooted. Wait for cluster to come back online."
        else
          echo "Rebooting node(s): {{.NODES}}"
          mise exec -- talosctl reboot --nodes {{.NODES}}
        fi

  talos:check-extensions:
    desc: Check if extensions are loaded on nodes
    cmds:
      - echo "Checking extensions on {{.NODE_1_IP}}..."
      - mise exec -- talosctl get extensions --nodes {{.NODE_1_IP}}
      - echo "Checking extensions on {{.NODE_2_IP}}..."
      - mise exec -- talosctl get extensions --nodes {{.NODE_2_IP}}
      - echo "Checking extensions on {{.NODE_3_IP}}..."
      - mise exec -- talosctl get extensions --nodes {{.NODE_3_IP}}

  talos:recover-kubeconfig:
    desc: Recover kubeconfig after cluster restart or certificate issues
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Recovering kubeconfig from cluster..."
      - mise exec -- talosctl kubeconfig --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}} --force
      - echo "Testing connection..."
      - mise exec -- kubectl get nodes || echo "If connection fails, check if nodes are powered on and reachable"

  talos:fix-cilium:
    desc: Fix Cilium CNI after cluster issues (for Talos without kube-proxy)
    cmds:
      - echo "Redeploying Cilium with correct Talos configuration..."
      - mise exec -- task apps:deploy-cilium
      - echo "Checking Cilium status..."
      - mise exec -- kubectl get pods -n kube-system | grep cilium
      - echo "Nodes should become Ready within a few minutes"

  cluster:recover:
    desc: Complete cluster recovery process after power outage or certificate issues
    cmds:
      - echo "Starting complete cluster recovery process..."
      - echo "Step 1 - Ensuring environment is configured..."
      - 'test -f .env || (echo "ERROR: .env file missing. Copy from .env.example and set OP_ACCOUNT" && exit 1)'
      - source .env
      - echo "Step 2 - Restoring secrets from 1Password..."
      - mise exec -- task talos:restore-secrets
      - echo "Step 3 - Regenerating Talos configuration..."
      - mise exec -- task talos:generate-config
      - echo "Step 4 - Recovering kubeconfig..."
      - mise exec -- task talos:recover-kubeconfig
      - echo "Step 5 - Checking cluster status..."
      - mise exec -- kubectl get nodes || echo "Nodes not ready yet"
      - echo "Step 6 - Fixing Cilium if needed..."
      - mise exec -- task talos:fix-cilium
      - echo "Step 7 - Final status check..."
      - sleep 60
      - mise exec -- kubectl get nodes
      - echo "Recovery process complete. All nodes should be Ready."

  # Flux tasks
  flux:bootstrap:
    desc: Bootstrap Flux GitOps with 1Password integration
    preconditions:
      - sh: 'command -v op'
        msg: '1Password CLI (op) is required to retrieve GitHub token'
    cmds:
      - echo "Bootstrapping Flux GitOps..."
      - |
        export GITHUB_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") && \
        mise exec -- flux bootstrap github \
          --owner=geoffdavis \
          --repository=talos-gitops \
          --branch=main \
          --path=clusters/homelab \
          --personal=true

  flux:reconcile:
    desc: Force Flux reconciliation
    cmds:
      - mise exec -- flux reconcile source git flux-system
      - mise exec -- flux reconcile kustomization flux-system

  # Application deployment
  apps:deploy-core:
    desc: Deploy core applications (before Flux)
    cmds:
      - mise exec -- task apps:deploy-cilium
      - mise exec -- task apps:deploy-external-secrets
      - mise exec -- task apps:deploy-onepassword-connect
      - mise exec -- task apps:deploy-longhorn

  apps:deploy-cilium:
    desc: Deploy Cilium CNI for Talos (with kube-proxy replacement)
    cmds:
      - mise exec -- helm repo add cilium https://helm.cilium.io/ || true
      - mise exec -- helm repo update
      - |
        mise exec -- helm upgrade --install cilium cilium/cilium \
            --version 1.16.1 \
            --namespace kube-system \
            --set ipam.mode=cluster-pool \
            --set ipam.operator.clusterPoolIPv4PodCIDRList="10.0.0.0/8" \
            --set ipam.operator.clusterPoolIPv4MaskSize=24 \
            --set kubeProxyReplacement=false \
            --set securityContext.privileged=true \
            --set cni.install=true \
            --set cni.exclusive=false \
            --set hubble.enabled=true \
            --set hubble.relay.enabled=true \
            --set hubble.ui.enabled=true \
            --set operator.replicas=1 \
            --set operator.rollOutPods=true \
            --set k8sServiceHost=localhost \
            --set k8sServicePort=7445 \
            --set tolerations[0].key=node-role.kubernetes.io/control-plane \
            --set tolerations[0].operator=Exists \
            --set tolerations[0].effect=NoSchedule \
            --set tolerations[1].key=node-role.kubernetes.io/master \
            --set tolerations[1].operator=Exists \
            --set tolerations[1].effect=NoSchedule \
            --set tolerations[2].key=node.kubernetes.io/not-ready \
            --set tolerations[2].operator=Exists \
            --set tolerations[2].effect=NoSchedule
      - echo "Waiting for Cilium to be ready..."
      - sleep 30
      - mise exec -- kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=cilium-agent -n kube-system --timeout=300s || true

  apps:deploy-external-secrets:
    desc: Deploy External Secrets Operator
    cmds:
      - mise exec -- helm repo add external-secrets https://charts.external-secrets.io || true
      - mise exec -- helm repo update
      - mise exec -- helm upgrade --install external-secrets external-secrets/external-secrets --namespace external-secrets-system --create-namespace --wait
      - echo "Waiting for External Secrets CRDs to be ready..."
      - mise exec -- kubectl wait --for condition=established --timeout=120s crd/clustersecretstores.external-secrets.io
      - mise exec -- kubectl wait --for condition=established --timeout=120s crd/secretstores.external-secrets.io
      - echo "Waiting for External Secrets webhook to be fully ready..."
      - mise exec -- kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=external-secrets-webhook -n external-secrets-system --timeout=120s
      - sleep 15

  apps:deploy-onepassword-connect:
    desc: Deploy 1Password Connect
    cmds:
      - echo "Deploying 1Password Connect basic resources..."
      - mise exec -- kubectl apply -f infrastructure/onepassword-connect/namespace.yaml
      - mise exec -- kubectl apply -f infrastructure/onepassword-connect/deployment.yaml
      - mise exec -- kubectl apply -f infrastructure/onepassword-connect/service.yaml
      - echo "Waiting for 1Password Connect to be ready..."
      - mise exec -- kubectl rollout status deployment onepassword-connect -n onepassword-connect --timeout=120s
      - echo "Waiting for External Secrets CRDs to be fully available..."
      - mise exec -- kubectl wait --for condition=established --timeout=60s crd/clustersecretstores.external-secrets.io
      - mise exec -- kubectl wait --for condition=established --timeout=60s crd/secretstores.external-secrets.io
      - echo "Fixing External Secrets webhook if needed..."
      - mise exec -- task apps:fix-external-secrets-webhook
      - echo "Deploying 1Password Connect secret stores..."
      - |
        # Apply secret stores with proper webhook validation
        echo "Applying secret stores..."
        if mise exec -- kubectl apply -f infrastructure/onepassword-connect/secret-store.yaml; then
          echo "✓ Secret stores deployed successfully"
        else
          echo "⚠ Secret store deployment failed, fixing webhook and retrying..."
          mise exec -- task apps:fix-external-secrets-webhook
          sleep 10
          mise exec -- kubectl apply -f infrastructure/onepassword-connect/secret-store.yaml
          echo "✓ Secret stores deployed after webhook fix"
        fi

  apps:fix-external-secrets-webhook:
    desc: Fix External Secrets webhook validation issues
    cmds:
      - echo "Checking External Secrets webhook status..."
      - mise exec -- kubectl get validatingwebhookconfiguration | grep -E "(secretstore|external)" || echo "No webhook configurations found"
      - echo "Reinstalling External Secrets to restore webhook configuration..."
      - mise exec -- helm upgrade --install external-secrets external-secrets/external-secrets --namespace external-secrets-system --reuse-values
      - echo "Waiting for webhook to be ready..."
      - mise exec -- kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=external-secrets-webhook -n external-secrets-system --timeout=120s
      - echo "✓ External Secrets webhook restored"

  apps:deploy-longhorn:
    desc: Deploy Longhorn storage
    cmds:
      - mise exec -- helm repo add longhorn https://charts.longhorn.io
      - mise exec -- helm repo update
      - mise exec -- helm upgrade --install longhorn longhorn/longhorn -n longhorn-system --create-namespace

  apps:verify-core-idempotency:
    desc: Verify that apps:deploy-core is idempotent and can be run multiple times safely
    cmds:
      - ./scripts/verify-core-idempotency.sh
    preconditions:
      - sh: 'mise exec -- kubectl get namespaces &> /dev/null'
        msg: 'Kubernetes cluster must be accessible'
      - sh: '[ -x scripts/verify-core-idempotency.sh ]'
        msg: 'Idempotency verification script must be executable'

  # BGP Configuration (Multiple Methods)
  bgp:configure-unifi:
    desc: Configure BGP on Unifi UDM Pro (via SSH script - legacy method)
    cmds:
      - echo "Configuring BGP on Unifi UDM Pro via SSH script..."
      - scp scripts/unifi-bgp-config.sh unifi-admin@udm-pro:/tmp/
      - ssh unifi-admin@udm-pro "chmod +x /tmp/unifi-bgp-config.sh && /tmp/unifi-bgp-config.sh"

  bgp:generate-config:
    desc: Generate BGP configuration file for UniFi UI upload (recommended method)
    cmds:
      - echo "BGP configuration file available at scripts/unifi-bgp-config.conf"
      - echo ""
      - echo "To upload this configuration:"
      - echo "1. Open UniFi Network UI"
      - echo "2. Go to Network > Settings > Routing > BGP"
      - echo "3. Click 'Upload Configuration'"
      - echo "4. Select scripts/unifi-bgp-config.conf"
      - echo "5. Apply the configuration"

  bgp:show-config:
    desc: Display BGP configuration file contents
    cmds:
      - echo "=== BGP Configuration for UniFi UDM Pro ==="
      - cat scripts/unifi-bgp-config.conf

  bgp:verify-peering:
    desc: Verify BGP peering status (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP peering status on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp summary'"

  bgp:verify-peering-ipv6:
    desc: Verify BGP IPv6 peering status (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP IPv6 peering status on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast summary'"
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast neighbors'"

  bgp:show-routes-ipv6:
    desc: Show BGP IPv6 routes (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP IPv6 routes on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast'"
      - ssh unifi-admin@udm-pro "vtysh -c 'show ipv6 route bgp'"

  # Network diagnostics
  network:check-lldp:
    desc: Check LLDP neighbors on all nodes
    cmds:
      - echo "Checking LLDP neighbors on {{.NODE_1_IP}}..."
      - mise exec -- talosctl list /sys/class/net --nodes {{.NODE_1_IP}}
      - echo "Checking LLDP neighbors on {{.NODE_2_IP}}..."
      - mise exec -- talosctl list /sys/class/net --nodes {{.NODE_2_IP}}
      - echo "Checking LLDP neighbors on {{.NODE_3_IP}}..."
      - mise exec -- talosctl list /sys/class/net --nodes {{.NODE_3_IP}}

  network:verify-lldpd-config:
    desc: Verify LLDPD ExtensionServiceConfig and service status on all nodes
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "=== Verifying LLDPD Configuration and Service Status ==="
      - echo ""
      - echo "Checking LLDPD ExtensionServiceConfig on all nodes..."
      - |
        mise exec -- talosctl get extensionserviceconfigs --nodes {{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}} | grep lldpd || echo "No LLDPD ExtensionServiceConfig found"
      - echo ""
      - echo "Checking LLDPD service status on all nodes..."
      - |
        mise exec -- talosctl get services --nodes {{.NODE_1_IP}},{{.NODE_2_IP}},{{.NODE_3_IP}} | grep lldpd || echo "No LLDPD service found"
      - echo ""
      - echo "Checking for LLDPD configuration file on all nodes..."
      - |
        echo "Node {{.NODE_1_IP}}:"
        mise exec -- talosctl read /usr/local/etc/lldpd/lldpd.conf --nodes {{.NODE_1_IP}} 2>/dev/null || echo "  LLDPD config file not found"
        echo "Node {{.NODE_2_IP}}:"
        mise exec -- talosctl read /usr/local/etc/lldpd/lldpd.conf --nodes {{.NODE_2_IP}} 2>/dev/null || echo "  LLDPD config file not found"
        echo "Node {{.NODE_3_IP}}:"
        mise exec -- talosctl read /usr/local/etc/lldpd/lldpd.conf --nodes {{.NODE_3_IP}} 2>/dev/null || echo "  LLDPD config file not found"
      - echo ""
      - echo "✓ LLDPD verification complete"
      - echo "Note - HEALTHY=false is normal for LLDPD (no health checks). RUNNING=true indicates success."

  network:check-usb:
    desc: Check USB device detection on all nodes
    cmds:
      - echo "Checking USB devices on {{.NODE_1_IP}}..."
      - mise exec -- talosctl list /sys/bus/usb/devices --nodes {{.NODE_1_IP}}
      - echo "Checking USB devices on {{.NODE_2_IP}}..."
      - mise exec -- talosctl list /sys/bus/usb/devices --nodes {{.NODE_2_IP}}
      - echo "Checking USB devices on {{.NODE_3_IP}}..."
      - mise exec -- talosctl list /sys/bus/usb/devices --nodes {{.NODE_3_IP}}

  network:check-ipv6:
    desc: Check IPv6 configuration on all nodes
    cmds:
      - echo "Checking IPv6 addresses on {{.NODE_1_IP}}..."
      - mise exec -- talosctl read /proc/net/if_inet6 --nodes {{.NODE_1_IP}}
      - echo "Checking IPv6 addresses on {{.NODE_2_IP}}..."
      - mise exec -- talosctl read /proc/net/if_inet6 --nodes {{.NODE_2_IP}}
      - echo "Checking IPv6 addresses on {{.NODE_3_IP}}..."
      - mise exec -- talosctl read /proc/net/if_inet6 --nodes {{.NODE_3_IP}}

  network:test-ipv6:
    desc: Test IPv6 connectivity from cluster nodes
    cmds:
      - echo "Testing IPv6 connectivity from {{.NODE_1_IP}}..."
      - mise exec -- talosctl read /proc/net/route --nodes {{.NODE_1_IP}}
      - echo "Testing IPv6 connectivity from {{.NODE_2_IP}}..."
      - mise exec -- talosctl read /proc/net/route --nodes {{.NODE_2_IP}}
      - echo "Testing IPv6 connectivity from {{.NODE_3_IP}}..."
      - mise exec -- talosctl read /proc/net/route --nodes {{.NODE_3_IP}}

  # Storage diagnostics
  storage:check-iscsi:
    desc: Check iSCSI configuration on all nodes
    cmds:
      - echo "Checking iSCSI on {{.NODE_1_IP}}..."
      - mise exec -- talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_1_IP}}
      - echo "Checking iSCSI on {{.NODE_2_IP}}..."
      - mise exec -- talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_2_IP}}
      - echo "Checking iSCSI on {{.NODE_3_IP}}..."
      - mise exec -- talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_3_IP}}

  storage:check-longhorn:
    desc: Check Longhorn storage mounts
    cmds:
      - mise exec -- kubectl get pods -n longhorn-system
      - mise exec -- kubectl get nodes -o custom-columns=NAME:.metadata.name,LONGHORN-READY:.status.conditions[?(@.type=="LonghornReady")].status

  # Testing tasks
  test:all:
    desc: Run all tests
    cmds:
      - mise exec -- task test:config
      - mise exec -- task test:connectivity
      - mise exec -- task test:extensions
      - mise exec -- task test:recovery

  test:config:
    desc: Test configuration validity
    cmds:
      - echo "Testing Talos configuration..."
      - mise exec -- talosctl validate --config talos/generated/controlplane.yaml
      - echo "Testing Kubernetes manifests..."
      - mise exec -- kubectl apply --dry-run=client -f clusters/homelab/

  test:connectivity:
    desc: Test cluster connectivity
    cmds:
      - echo "Testing cluster connectivity..."
      - mise exec -- kubectl get nodes
      - mise exec -- kubectl get pods --all-namespaces

  test:extensions:
    desc: Test Talos extensions including LLDPD configuration
    cmds:
      - echo "Testing Talos extensions..."
      - mise exec -- task talos:check-extensions
      - mise exec -- task network:verify-lldpd-config
      - mise exec -- task network:check-usb
      - mise exec -- task storage:check-iscsi

  test:ipv6:
    desc: Test IPv6 configuration and connectivity
    cmds:
      - echo "Testing IPv6 configuration..."
      - mise exec -- task network:check-ipv6
      - mise exec -- task bgp:verify-peering-ipv6
      - mise exec -- kubectl get svc -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,IPv4:.status.loadBalancer.ingress[*].ip,IPv6:.status.loadBalancer.ingress[*].ip

  test:recovery:
    desc: Test cluster recovery functionality
    cmds:
      - echo "Testing cluster recovery functionality..."
      - ./tests/test-recovery.sh

  # Maintenance tasks
  maintenance:backup:
    desc: Backup cluster configuration
    cmds:
      - mkdir -p backups/$(date +%Y%m%d-%H%M%S)
      - mise exec -- kubectl get all --all-namespaces -o yaml > backups/$(date +%Y%m%d-%H%M%S)/cluster-state.yaml
      - mise exec -- talosctl get machineconfig -o yaml > backups/$(date +%Y%m%d-%H%M%S)/talos-config.yaml

  maintenance:cleanup:
    desc: Cleanup old resources
    cmds:
      - mise exec -- kubectl delete pods --field-selector=status.phase=Succeeded --all-namespaces
      - mise exec -- kubectl delete pods --field-selector=status.phase=Failed --all-namespaces

  # Development tasks
  dev:port-forward:
    desc: Port forward common services
    cmds:
      - mise exec -- kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80 &
      - mise exec -- kubectl port-forward -n onepassword-connect svc/onepassword-connect 8081:8080 &
      - echo "Services available at localhost:8080 (Longhorn) and localhost:8081 (1Password Connect)"

  dev:logs:
    desc: Tail logs for debugging
    cmds:
      - mise exec -- kubectl logs -f -n kube-system -l app.kubernetes.io/name=cilium

  # Cluster status
  cluster:status:
    desc: Show comprehensive cluster status for all-control-plane cluster
    cmds:
      - echo "=== All-Control-Plane Cluster Nodes ==="
      - mise exec -- kubectl get nodes -o wide
      - echo ""
      - echo "=== Node Roles (should show 'control-plane' for all nodes) ==="
      - mise exec -- kubectl get nodes --show-labels | grep "node-role.kubernetes.io"
      - echo ""
      - echo "=== etcd Members ==="
      - mise exec -- kubectl get pods -n kube-system -l component=etcd -o wide
      - echo ""
      - echo "=== Control Plane Components ==="
      - mise exec -- kubectl get pods -n kube-system -l tier=control-plane -o wide
      - echo ""
      - echo "=== System Pods ==="
      - mise exec -- kubectl get pods -n kube-system
      - echo ""
      - echo "=== Longhorn Status ==="
      - mise exec -- kubectl get pods -n longhorn-system
      - echo ""
      - echo "=== BGP Status ==="
      - mise exec -- kubectl get ciliumbgpclusterconfig
      - echo ""
      - echo "=== LoadBalancer Services ==="
      - mise exec -- kubectl get svc --all-namespaces | grep LoadBalancer

  # Renovate tasks
  renovate:dry-run:
    desc: Run Renovate in dry-run mode to see what updates are available
    cmds:
      - echo "Running Renovate dry-run to check for available updates..."
      - RENOVATE_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") npx renovate --dry-run geoffdavis/talos-gitops
    preconditions:
      - sh: 'command -v op'
        msg: '1Password CLI (op) is required to retrieve GitHub token'

  renovate:run:
    desc: Run Renovate locally (creates actual PRs)
    cmds:
      - echo "Running Renovate to create update PRs..."
      - echo "WARNING - This will create actual PRs if updates are found"
      - echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
      - sleep 5
      - RENOVATE_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") npx renovate geoffdavis/talos-gitops
    preconditions:
      - sh: 'command -v op'
        msg: '1Password CLI (op) is required to retrieve GitHub token'

  renovate:validate:
    desc: Validate Renovate configuration
    cmds:
      - echo "Validating Renovate configuration..."
      - npx renovate-config-validator .renovaterc.json

  renovate:install:
    desc: Install Renovate CLI globally
    cmds:
      - echo "Installing Renovate CLI..."
      - npm install -g renovate

  # 1Password Connect automation
  onepassword:create-connect-server:
    desc: Create new 1Password Connect server with proper credentials and store in 1Password
    preconditions:
      - sh: 'command -v op'
        msg: '1Password CLI (op) is required'
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: 'OP_ACCOUNT environment variable must be set'
    cmds:
      - echo "Creating new 1Password Connect server with credentials..."
      - |
        # Create Connect server with proper vault access
        echo "Creating Connect server for home-ops cluster..."
        op connect server create "home-ops-cluster" --vaults "Automation,Services" > /dev/null
        
        # Verify credentials file was created
        if [ ! -f "1password-credentials.json" ]; then
          echo "❌ Credentials file not created"
          exit 1
        fi
        
        # Validate JSON format
        if ! jq . 1password-credentials.json > /dev/null; then
          echo "❌ Invalid credentials JSON format"
          exit 1
        fi
        
        echo "✓ Valid credentials file created ($(wc -c < 1password-credentials.json) bytes)"
      - |
        # Store credentials as document in 1Password
        echo "Storing credentials in 1Password as document..."
        op document create 1password-credentials.json \
          --title="1Password Connect Credentials - home-ops" \
          --vault="Automation" > /dev/null
        echo "✓ Credentials stored as document in 1Password"
      - |
        # Create Connect token
        echo "Creating Connect token..."
        CONNECT_TOKEN=$(op connect token create "home-ops-token" \
          --server "home-ops-cluster" \
          --vault "Automation" \
          --expires-in 8760h)
        
        if [ ${#CONNECT_TOKEN} -lt 100 ]; then
          echo "❌ Connect token appears invalid (length: ${#CONNECT_TOKEN})"
          exit 1
        fi
        
        echo "✓ Connect token created (length: ${#CONNECT_TOKEN})"
        
        # Store token in 1Password
        echo "Storing Connect token in 1Password..."
        op item create \
          --category="API Credential" \
          --title="1Password Connect Token - home-ops" \
          --vault="Automation" \
          "token[password]=$CONNECT_TOKEN" > /dev/null || \
        op item edit "1Password Connect Token - home-ops" \
          "token[password]=$CONNECT_TOKEN" \
          --vault="Automation" > /dev/null
        echo "✓ Connect token stored in 1Password"
      - |
        # Update Kubernetes secrets
        echo "Updating Kubernetes secrets..."
        mise exec -- kubectl create secret generic onepassword-connect-credentials \
          --from-file=1password-credentials.json \
          --namespace=onepassword-connect \
          --dry-run=client -o yaml | mise exec -- kubectl apply -f -
        
        CONNECT_TOKEN=$(op connect token create "home-ops-token" \
          --server "home-ops-cluster" \
          --vault "Automation" \
          --expires-in 8760h)
        
        mise exec -- kubectl create secret generic onepassword-connect-token \
          --from-literal=token="$CONNECT_TOKEN" \
          --namespace=onepassword-connect \
          --dry-run=client -o yaml | mise exec -- kubectl apply -f -
        
        echo "✓ Kubernetes secrets updated"
      - |
        # Restart 1Password Connect deployment
        echo "Restarting 1Password Connect deployment..."
        mise exec -- kubectl rollout restart deployment onepassword-connect -n onepassword-connect
        mise exec -- kubectl rollout status deployment onepassword-connect -n onepassword-connect --timeout=120s
        echo "✓ 1Password Connect deployment restarted"
      - |
        # Clean up local files
        echo "Cleaning up local files..."
        rm -f 1password-credentials.json connect-token.txt
        echo "✓ Local files cleaned up"
      - echo "✅ 1Password Connect server setup completed successfully!"

  onepassword:fix-credentials:
    desc: Fix 1Password Connect credentials truncation issue
    deps: [onepassword:create-connect-server]
    cmds:
      - echo "1Password Connect credentials have been regenerated and updated"
      - mise exec -- task bootstrap:validate-1password-secrets

  # Cluster Safety Tasks - CRITICAL SAFETY MEASURES
  cluster:safe-reset:
    desc: SAFE cluster reset - only resets EPHEMERAL and STATE partitions (preserves OS)
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "⚠️  CLUSTER SAFE RESET - This will reset EPHEMERAL and STATE partitions only"
      - echo "⚠️  This operation will:"
      - echo "   - Reset user data and configurations"
      - echo "   - Preserve the Talos OS installation"
      - echo "   - Require cluster re-bootstrap after reset"
      - echo ""
      - echo "❌ NEVER use 'talosctl reset' without partition specifications!"
      - echo "❌ That command WILL WIPE THE ENTIRE OS and require USB reinstallation!"
      - echo ""
      - |
        read -p "Type 'SAFE-RESET' to confirm this safe reset operation: " confirm
        [ "$confirm" = "SAFE-RESET" ]
      - echo "Performing SAFE reset on all nodes (EPHEMERAL and STATE partitions only)..."
      - |
        echo "Resetting node {{.NODE_1_IP}} (SAFE - partitions only)..."
        mise exec -- talosctl reset --nodes {{.NODE_1_IP}} --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --reboot
      - sleep 30
      - |
        echo "Resetting node {{.NODE_2_IP}} (SAFE - partitions only)..."
        mise exec -- talosctl reset --nodes {{.NODE_2_IP}} --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --reboot
      - sleep 30
      - |
        echo "Resetting node {{.NODE_3_IP}} (SAFE - partitions only)..."
        mise exec -- talosctl reset --nodes {{.NODE_3_IP}} --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --reboot
      - echo "✅ SAFE reset completed. OS preserved. Cluster will need re-bootstrap."
      - echo "Next steps - Run 'mise exec -- task bootstrap:cluster' to rebuild the cluster"

  cluster:emergency-recovery:
    desc: Emergency cluster recovery with comprehensive safety checks
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "🚨 EMERGENCY CLUSTER RECOVERY PROCEDURE"
      - echo "This will attempt to recover the cluster using safe methods only"
      - echo ""
      - |
        read -p "Confirm emergency recovery (y/N): " confirm
        [ "$confirm" = "y" ]
      - echo "Step 1 - Verifying cluster safety..."
      - mise exec -- task cluster:verify-safety
      - echo "Step 2 - Attempting kubeconfig recovery..."
      - mise exec -- task talos:recover-kubeconfig
      - echo "Step 3 - Checking node status..."
      - mise exec -- kubectl get nodes || echo "Nodes not accessible - may need safe reset"
      - echo "Step 4 - Checking critical pods..."
      - mise exec -- kubectl get pods -n kube-system || echo "Control plane not accessible"
      - echo "Step 5 - If cluster is unrecoverable, use mise exec -- task cluster:safe-reset"
      - echo "✅ Emergency recovery assessment complete"

  cluster:safe-reboot:
    desc: Safe cluster reboot (alternative to reset operations)
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "🔄 SAFE CLUSTER REBOOT"
      - echo "This will reboot all nodes safely without data loss"
      - echo ""
      - |
        read -p "Confirm safe reboot of all nodes (y/N): " confirm
        [ "$confirm" = "y" ]
      - echo "Rebooting all nodes safely..."
      - |
        echo "Rebooting {{.NODE_1_IP}}..."
        mise exec -- talosctl reboot --nodes {{.NODE_1_IP}}
        sleep 45
      - |
        echo "Rebooting {{.NODE_2_IP}}..."
        mise exec -- talosctl reboot --nodes {{.NODE_2_IP}}
        sleep 45
      - |
        echo "Rebooting {{.NODE_3_IP}}..."
        mise exec -- talosctl reboot --nodes {{.NODE_3_IP}}
        sleep 45
      - echo "Waiting for cluster to come back online..."
      - sleep 60
      - mise exec -- task talos:recover-kubeconfig
      - mise exec -- kubectl get nodes
      - echo "✅ Safe reboot completed"

  cluster:verify-safety:
    desc: Verify cluster safety and warn about dangerous operations
    cmds:
      - echo "🔍 CLUSTER SAFETY VERIFICATION"
      - echo ""
      - echo "✅ SAFE OPERATIONS:"
      - echo "   - talosctl reset --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL"
      - echo "   - talosctl reboot"
      - echo "   - talosctl upgrade"
      - echo "   - task cluster:safe-reset"
      - echo "   - task cluster:safe-reboot"
      - echo ""
      - echo "❌ DANGEROUS OPERATIONS (NEVER USE):"
      - echo "   - talosctl reset (without partition specifications)"
      - echo "   - Any reset command that doesn't specify partitions"
      - echo ""
      - echo "📋 CURRENT CLUSTER STATUS:"
      - mise exec -- kubectl get nodes -o wide || echo "Cluster not accessible"
      - echo ""
      - echo "📖 For detailed safety information, see:"
      - echo "   - docs/CLUSTER_RESET_SAFETY.md"
      - echo "   - docs/SUBTASK_SAFETY_GUIDELINES.md"