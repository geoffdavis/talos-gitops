version: '3'

dotenv: ['.env']

vars:
  CLUSTER_NAME: home-ops
  CONTROL_PLANE_SUBNET: 172.29.51.0/24
  # renovate: datasource=github-releases depName=siderolabs/talos
  TALOS_VERSION: v1.10.5
  # renovate: datasource=github-releases depName=kubernetes/kubernetes
  KUBERNETES_VERSION: v1.31.1
  CLUSTER_ENDPOINT: https://172.29.51.10:6443
  NODE_1_IP: 172.29.51.11
  NODE_2_IP: 172.29.51.12
  NODE_3_IP: 172.29.51.13
  TALOSCONFIG: talos/generated/talosconfig

tasks:
  # Bootstrap tasks
  bootstrap:secrets:
    desc: Bootstrap secrets from 1Password (requires OP_ACCOUNT environment variable)
    cmds:
      - ./scripts/bootstrap-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'

  bootstrap:k8s-secrets:
    desc: Bootstrap Kubernetes secrets from 1Password (for existing cluster)
    cmds:
      - ./scripts/bootstrap-k8s-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'
      - sh: 'kubectl get namespaces &> /dev/null'
        msg: 'Kubernetes cluster must be accessible'

  bootstrap:1password-secrets:
    desc: Bootstrap 1Password Connect secrets for fresh cluster (requires OP_ACCOUNT environment variable)
    cmds:
      - ./scripts/bootstrap-1password-secrets.sh
    preconditions:
      - sh: '[ -n "$OP_ACCOUNT" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'

  bootstrap:validate-1password-secrets:
    desc: Validate 1Password Connect secrets were created correctly
    cmds:
      - ./scripts/validate-1password-secrets.sh

  bootstrap:cluster:
    desc: Bootstrap the entire cluster from scratch
    deps: [bootstrap:secrets, talos:generate-config]
    cmds:
      - task: talos:apply-config
      - task: talos:bootstrap
      - task: bootstrap:1password-secrets
      - task: bootstrap:validate-1password-secrets
      - task: flux:bootstrap
      - task: apps:deploy-core

  # Talos tasks
  talos:generate-schematic:
    desc: Generate custom Talos schematic with extensions via Image Factory
    cmds:
      - ./scripts/generate-talos-schematic.sh
    preconditions:
      - sh: 'command -v curl'
        msg: 'curl is required for Image Factory API'
      - sh: 'command -v jq'
        msg: 'jq is required for JSON processing'

  talos:update-installer-images:
    desc: Update machine configurations to use custom installer images
    deps: [talos:generate-schematic]
    cmds:
      - ./scripts/update-installer-images.sh
    preconditions:
      - sh: '[ -f talos/generated/schematic-id.txt ]'
        msg: 'Schematic ID file not found. Run task talos:generate-schematic first.'

  talos:restore-secrets:
    desc: Restore Talos secrets from the correct 1Password entry (talos - home-ops)
    preconditions:
      - sh: '[ -n "${OP_ACCOUNT:-}" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'
    cmds:
      - echo "Restoring Talos secrets from 1Password 'talos - {{.CLUSTER_NAME}}' entry..."
      - |
        op --account {{.OP_ACCOUNT}} item get "talos - {{.CLUSTER_NAME}}" --vault="Automation" --format json | jq -r '{
          cluster: {
            id: .fields[] | select(.label == "cluster_id") | .value,
            secret: .fields[] | select(.label == "cluster_secret") | .value
          },
          secrets: {
            bootstraptoken: .fields[] | select(.label == "bootstraptoken") | .value,
            secretboxencryptionsecret: .fields[] | select(.label == "secretboxencryptionsecret") | .value
          },
          trustdinfo: {
            token: .fields[] | select(.label == "trustdinfo_token") | .value
          },
          certs: {
            etcd: {
              crt: .fields[] | select(.label == "cert_etcd_crt") | .value,
              key: .fields[] | select(.label == "cert_etcd_key") | .value
            },
            k8s: {
              crt: .fields[] | select(.label == "cert_k8s_crt") | .value,
              key: .fields[] | select(.label == "cert_k8s_key") | .value
            },
            k8saggregator: {
              crt: .fields[] | select(.label == "cert_k8saggregator_crt") | .value,
              key: .fields[] | select(.label == "cert_k8saggregator_key") | .value
            },
            k8sserviceaccount: {
              key: .fields[] | select(.label == "cert_k8sserviceaccount_key") | .value
            },
            os: {
              crt: .fields[] | select(.label == "cert_os_crt") | .value,
              key: .fields[] | select(.label == "cert_os_key") | .value
            }
          }
        }' | yq -P > talos/talsecret.yaml
      - echo "Secrets restored from 1Password"

  talos:generate-config:
    desc: Generate Talos configuration using talhelper with 1Password integration
    preconditions:
      - sh: 'command -v talhelper'
        msg: 'talhelper is required. Install with: go install github.com/budimanjojo/talhelper/cmd/talhelper@latest'
      - sh: '[ -n "${OP_ACCOUNT:-}" ]'
        msg: 'OP_ACCOUNT environment variable must be set (e.g., export OP_ACCOUNT=YourAccountName)'
    cmds:
      - echo "Generating Talos configuration with talhelper and 1Password integration..."
      - mkdir -p talos/generated
      - |
        # Check for existing secrets in multiple locations
        # First check for "talos - home-ops" (legacy entry)
        if op --account {{.OP_ACCOUNT}} item get "talos - {{.CLUSTER_NAME}}" --vault="Automation" &> /dev/null; then
          echo "Found legacy secrets entry 'talos - {{.CLUSTER_NAME}}', restoring..."
          task talos:restore-secrets
        # Then check for "Talos Secrets - home-ops" (new format)
        elif op item get "Talos Secrets - {{.CLUSTER_NAME}}" &> /dev/null; then
          echo "Retrieving existing secrets from 1Password..."
          op item get "Talos Secrets - {{.CLUSTER_NAME}}" --fields label=talsecret --format json | jq -r '.value' > talos/talsecret.yaml
          echo "Using existing secrets from 1Password"
        elif [[ -f talos/talsecret.yaml ]]; then
          echo "Using existing local secrets file..."
        else
          echo "Generating new Talos secrets..."
          talhelper gensecret > talos/talsecret.yaml
          
          # Store new secrets in 1Password
          echo "Storing new secrets in 1Password..."
          if ! op item get "Talos Secrets - {{.CLUSTER_NAME}}" &> /dev/null; then
            op item create \
              --category="Secure Note" \
              --title="Talos Secrets - {{.CLUSTER_NAME}}" \
              --vault="Automation" \
              "talsecret[password]=$(cat talos/talsecret.yaml)"
          else
            op item edit "Talos Secrets - {{.CLUSTER_NAME}}" \
              "talsecret[password]=$(cat talos/talsecret.yaml)"
          fi
          echo "Secrets stored in 1Password"
        fi
      - |
        # Generate configuration files
        echo "Generating Talos configuration files..."
        talhelper genconfig --secret-file talos/talsecret.yaml
      - echo "Generated Talos configuration with talhelper"
      - echo "Configuration files created in clusterconfig/"

  talos:apply-config:
    desc: Apply Talos configuration to nodes
    deps: [talos:generate-config]
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Applying Talos configuration to all control plane nodes..."
      - |
        echo "Applying configuration to mini01 ({{.NODE_1_IP}})..."
        if talosctl apply-config --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml; then
          echo "✓ mini01 configuration applied successfully"
        elif talosctl apply-config --insecure --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml; then
          echo "✓ mini01 configuration applied successfully (insecure mode)"
        else
          echo "✗ Failed to apply configuration to mini01"
          exit 1
        fi
      - |
        echo "Applying configuration to mini02 ({{.NODE_2_IP}})..."
        if talosctl apply-config --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml; then
          echo "✓ mini02 configuration applied successfully"
        elif talosctl apply-config --insecure --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml; then
          echo "✓ mini02 configuration applied successfully (insecure mode)"
        else
          echo "✗ Failed to apply configuration to mini02"
          exit 1
        fi
      - |
        echo "Applying configuration to mini03 ({{.NODE_3_IP}})..."
        if talosctl apply-config --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml; then
          echo "✓ mini03 configuration applied successfully"
        elif talosctl apply-config --insecure --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml; then
          echo "✓ mini03 configuration applied successfully (insecure mode)"
        else
          echo "✗ Failed to apply configuration to mini03"
          exit 1
        fi
      - echo "✓ Configuration successfully applied to all nodes"

  talos:apply-config-only:
    desc: Apply Talos configuration to nodes (without regenerating)
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Applying Talos configuration to all control plane nodes..."
      - |
        # Try with talosconfig first, fall back to insecure if needed
        talosctl apply-config --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml || \
        talosctl apply-config --insecure --nodes {{.NODE_1_IP}} --file clusterconfig/home-ops-mini01.yaml
      - |
        talosctl apply-config --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml || \
        talosctl apply-config --insecure --nodes {{.NODE_2_IP}} --file clusterconfig/home-ops-mini02.yaml
      - |
        talosctl apply-config --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml || \
        talosctl apply-config --insecure --nodes {{.NODE_3_IP}} --file clusterconfig/home-ops-mini03.yaml
      - echo "Configuration applied to all nodes"

  talos:bootstrap:
    desc: Bootstrap Talos cluster with all control plane nodes
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Bootstrapping first control plane node..."
      - talosctl bootstrap --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}}
      - echo "Waiting for cluster to initialize..."
      - sleep 30
      - echo "Getting kubeconfig from cluster..."
      - talosctl kubeconfig --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}} --force
      - echo "All three nodes are now functioning as control plane nodes"

  talos:convert-to-all-controlplane:
    desc: Convert existing cluster to all-control-plane setup
    deps: [talos:generate-config]
    cmds:
      - echo "Converting cluster to all-control-plane setup..."
      - echo "Applying control plane configuration to all nodes..."
      - talosctl apply-config --nodes {{.NODE_1_IP}} --file talos/generated/controlplane.yaml
      - talosctl apply-config --nodes {{.NODE_2_IP}} --file talos/generated/controlplane.yaml
      - talosctl apply-config --nodes {{.NODE_3_IP}} --file talos/generated/controlplane.yaml
      - echo "Waiting for nodes to restart and join as control planes..."
      - sleep 60
      - echo "Verifying cluster status..."
      - kubectl get nodes -o wide
      - echo "All nodes should now show as control planes with 'control-plane' role"

  talos:upgrade:
    desc: Upgrade Talos nodes
    cmds:
      - talosctl upgrade --nodes {{.NODE_1_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}
      - talosctl upgrade --nodes {{.NODE_2_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}
      - talosctl upgrade --nodes {{.NODE_3_IP}} --image ghcr.io/siderolabs/talos:{{.TALOS_VERSION}}

  talos:reboot:
    desc: Reboot specified nodes or all nodes (for USB detection)
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    vars:
      NODES: '{{.NODES | default "all"}}'
    cmds:
      - |
        if [ "{{.NODES}}" = "all" ]; then
          echo "Rebooting all nodes for USB device detection..."
          talosctl reboot --nodes {{.NODE_1_IP}}
          sleep 30
          talosctl reboot --nodes {{.NODE_2_IP}}
          sleep 30
          talosctl reboot --nodes {{.NODE_3_IP}}
          echo "All nodes rebooted. Wait for cluster to come back online."
        else
          echo "Rebooting node(s): {{.NODES}}"
          talosctl reboot --nodes {{.NODES}}
        fi

  talos:check-extensions:
    desc: Check if extensions are loaded on nodes
    cmds:
      - echo "Checking extensions on {{.NODE_1_IP}}..."
      - talosctl get extensions --nodes {{.NODE_1_IP}}
      - echo "Checking extensions on {{.NODE_2_IP}}..."
      - talosctl get extensions --nodes {{.NODE_2_IP}}
      - echo "Checking extensions on {{.NODE_3_IP}}..."
      - talosctl get extensions --nodes {{.NODE_3_IP}}

  talos:recover-kubeconfig:
    desc: Recover kubeconfig after cluster restart or certificate issues
    env:
      TALOSCONFIG: clusterconfig/talosconfig
    cmds:
      - echo "Recovering kubeconfig from cluster..."
      - talosctl kubeconfig --nodes {{.NODE_1_IP}} --endpoints {{.NODE_1_IP}} --force
      - echo "Testing connection..."
      - kubectl get nodes || echo "If connection fails, check if nodes are powered on and reachable"

  talos:fix-cilium:
    desc: Fix Cilium CNI after cluster issues (for Talos without kube-proxy)
    cmds:
      - echo "Redeploying Cilium with correct Talos configuration..."
      - task: apps:deploy-cilium
      - echo "Checking Cilium status..."
      - kubectl get pods -n kube-system | grep cilium
      - echo "Nodes should become Ready within a few minutes"

  cluster:recover:
    desc: Complete cluster recovery process after power outage or certificate issues
    cmds:
      - echo "Starting complete cluster recovery process..."
      - echo "Step 1 - Ensuring environment is configured..."
      - 'test -f .env || (echo "ERROR: .env file missing. Copy from .env.example and set OP_ACCOUNT" && exit 1)'
      - source .env
      - echo "Step 2 - Restoring secrets from 1Password..."
      - task: talos:restore-secrets
      - echo "Step 3 - Regenerating Talos configuration..."
      - task: talos:generate-config
      - echo "Step 4 - Recovering kubeconfig..."
      - task: talos:recover-kubeconfig
      - echo "Step 5 - Checking cluster status..."
      - kubectl get nodes || echo "Nodes not ready yet"
      - echo "Step 6 - Fixing Cilium if needed..."
      - task: talos:fix-cilium
      - echo "Step 7 - Final status check..."
      - sleep 60
      - kubectl get nodes
      - echo "Recovery process complete. All nodes should be Ready."

  # Flux tasks
  flux:bootstrap:
    desc: Bootstrap Flux GitOps with 1Password integration
    preconditions:
      - sh: 'command -v op'
        msg: '1Password CLI (op) is required to retrieve GitHub token'
    cmds:
      - echo "Bootstrapping Flux GitOps..."
      - |
        export GITHUB_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") && \
        flux bootstrap github \
          --owner=geoffdavis \
          --repository=talos-gitops \
          --branch=main \
          --path=clusters/homelab \
          --personal=true

  flux:reconcile:
    desc: Force Flux reconciliation
    cmds:
      - flux reconcile source git flux-system
      - flux reconcile kustomization flux-system

  # Application deployment
  apps:deploy-core:
    desc: Deploy core applications
    cmds:
      - task: apps:deploy-cilium
      - task: apps:deploy-onepassword-connect
      - task: apps:deploy-longhorn
      - task: apps:deploy-ingress

  apps:deploy-cilium:
    desc: Deploy Cilium CNI for Talos (with kube-proxy replacement)
    cmds:
      - helm repo add cilium https://helm.cilium.io/ || true
      - helm repo update
      - |
        helm upgrade --install cilium cilium/cilium \
            --version 1.16.1 \
            --namespace kube-system \
            --set ipam.mode=cluster-pool \
            --set ipam.operator.clusterPoolIPv4PodCIDRList="10.0.0.0/8" \
            --set ipam.operator.clusterPoolIPv4MaskSize=24 \
            --set kubeProxyReplacement=false \
            --set securityContext.privileged=true \
            --set cni.install=true \
            --set cni.exclusive=false \
            --set hubble.enabled=true \
            --set hubble.relay.enabled=true \
            --set hubble.ui.enabled=true \
            --set operator.replicas=1 \
            --set operator.rollOutPods=true \
            --set k8sServiceHost=localhost \
            --set k8sServicePort=7445 \
            --set tolerations[0].key=node-role.kubernetes.io/control-plane \
            --set tolerations[0].operator=Exists \
            --set tolerations[0].effect=NoSchedule \
            --set tolerations[1].key=node-role.kubernetes.io/master \
            --set tolerations[1].operator=Exists \
            --set tolerations[1].effect=NoSchedule \
            --set tolerations[2].key=node.kubernetes.io/not-ready \
            --set tolerations[2].operator=Exists \
            --set tolerations[2].effect=NoSchedule
      - echo "Waiting for Cilium to be ready..."
      - sleep 30
      - kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=cilium-agent -n kube-system --timeout=300s || true

  apps:deploy-onepassword-connect:
    desc: Deploy 1Password Connect
    cmds:
      - kubectl apply -f infrastructure/onepassword-connect/

  apps:deploy-longhorn:
    desc: Deploy Longhorn storage
    cmds:
      - helm repo add longhorn https://charts.longhorn.io
      - helm repo update
      - helm upgrade --install longhorn longhorn/longhorn -n longhorn-system --create-namespace -f infrastructure/longhorn/values.yaml

  apps:deploy-ingress:
    desc: Deploy ingress controller
    cmds:
      - kubectl apply -f infrastructure/ingress-nginx/

  # BGP Configuration (Multiple Methods)
  bgp:configure-unifi:
    desc: Configure BGP on Unifi UDM Pro (via SSH script - legacy method)
    cmds:
      - echo "Configuring BGP on Unifi UDM Pro via SSH script..."
      - scp scripts/unifi-bgp-config.sh unifi-admin@udm-pro:/tmp/
      - ssh unifi-admin@udm-pro "chmod +x /tmp/unifi-bgp-config.sh && /tmp/unifi-bgp-config.sh"

  bgp:generate-config:
    desc: Generate BGP configuration file for UniFi UI upload (recommended method)
    cmds:
      - echo "BGP configuration file available at scripts/unifi-bgp-config.conf"
      - echo ""
      - echo "To upload this configuration:"
      - echo "1. Open UniFi Network UI"
      - echo "2. Go to Network > Settings > Routing > BGP"
      - echo "3. Click 'Upload Configuration'"
      - echo "4. Select scripts/unifi-bgp-config.conf"
      - echo "5. Apply the configuration"

  bgp:show-config:
    desc: Display BGP configuration file contents
    cmds:
      - echo "=== BGP Configuration for UniFi UDM Pro ==="
      - cat scripts/unifi-bgp-config.conf

  bgp:verify-peering:
    desc: Verify BGP peering status (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP peering status on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp summary'"

  bgp:verify-peering-ipv6:
    desc: Verify BGP IPv6 peering status (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP IPv6 peering status on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast summary'"
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast neighbors'"

  bgp:show-routes-ipv6:
    desc: Show BGP IPv6 routes (requires SSH access to UDM Pro)
    cmds:
      - echo "Checking BGP IPv6 routes on UDM Pro..."
      - ssh unifi-admin@udm-pro "vtysh -c 'show bgp ipv6 unicast'"
      - ssh unifi-admin@udm-pro "vtysh -c 'show ipv6 route bgp'"

  # Network diagnostics
  network:check-lldp:
    desc: Check LLDP neighbors on all nodes
    cmds:
      - echo "Checking LLDP neighbors on {{.NODE_1_IP}}..."
      - talosctl list /sys/class/net --nodes {{.NODE_1_IP}}
      - echo "Checking LLDP neighbors on {{.NODE_2_IP}}..."
      - talosctl list /sys/class/net --nodes {{.NODE_2_IP}}
      - echo "Checking LLDP neighbors on {{.NODE_3_IP}}..."
      - talosctl list /sys/class/net --nodes {{.NODE_3_IP}}

  network:check-usb:
    desc: Check USB device detection on all nodes
    cmds:
      - echo "Checking USB devices on {{.NODE_1_IP}}..."
      - talosctl list /sys/bus/usb/devices --nodes {{.NODE_1_IP}}
      - echo "Checking USB devices on {{.NODE_2_IP}}..."
      - talosctl list /sys/bus/usb/devices --nodes {{.NODE_2_IP}}
      - echo "Checking USB devices on {{.NODE_3_IP}}..."
      - talosctl list /sys/bus/usb/devices --nodes {{.NODE_3_IP}}

  network:check-ipv6:
    desc: Check IPv6 configuration on all nodes
    cmds:
      - echo "Checking IPv6 addresses on {{.NODE_1_IP}}..."
      - talosctl read /proc/net/if_inet6 --nodes {{.NODE_1_IP}}
      - echo "Checking IPv6 addresses on {{.NODE_2_IP}}..."
      - talosctl read /proc/net/if_inet6 --nodes {{.NODE_2_IP}}
      - echo "Checking IPv6 addresses on {{.NODE_3_IP}}..."
      - talosctl read /proc/net/if_inet6 --nodes {{.NODE_3_IP}}

  network:test-ipv6:
    desc: Test IPv6 connectivity from cluster nodes
    cmds:
      - echo "Testing IPv6 connectivity from {{.NODE_1_IP}}..."
      - talosctl read /proc/net/route --nodes {{.NODE_1_IP}}
      - echo "Testing IPv6 connectivity from {{.NODE_2_IP}}..."
      - talosctl read /proc/net/route --nodes {{.NODE_2_IP}}
      - echo "Testing IPv6 connectivity from {{.NODE_3_IP}}..."
      - talosctl read /proc/net/route --nodes {{.NODE_3_IP}}

  # Storage diagnostics
  storage:check-iscsi:
    desc: Check iSCSI configuration on all nodes
    cmds:
      - echo "Checking iSCSI on {{.NODE_1_IP}}..."
      - talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_1_IP}}
      - echo "Checking iSCSI on {{.NODE_2_IP}}..."
      - talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_2_IP}}
      - echo "Checking iSCSI on {{.NODE_3_IP}}..."
      - talosctl read /etc/iscsi/iscsid.conf --nodes {{.NODE_3_IP}}

  storage:check-longhorn:
    desc: Check Longhorn storage mounts
    cmds:
      - kubectl get pods -n longhorn-system
      - kubectl get nodes -o custom-columns=NAME:.metadata.name,LONGHORN-READY:.status.conditions[?(@.type=="LonghornReady")].status

  # Testing tasks
  test:all:
    desc: Run all tests
    cmds:
      - task: test:config
      - task: test:connectivity
      - task: test:extensions
      - task: test:recovery

  test:config:
    desc: Test configuration validity
    cmds:
      - echo "Testing Talos configuration..."
      - talosctl validate --config talos/generated/controlplane.yaml
      - echo "Testing Kubernetes manifests..."
      - kubectl apply --dry-run=client -f clusters/homelab/

  test:connectivity:
    desc: Test cluster connectivity
    cmds:
      - echo "Testing cluster connectivity..."
      - kubectl get nodes
      - kubectl get pods --all-namespaces

  test:extensions:
    desc: Test Talos extensions
    cmds:
      - echo "Testing Talos extensions..."
      - task: talos:check-extensions
      - task: network:check-usb
      - task: storage:check-iscsi

  test:ipv6:
    desc: Test IPv6 configuration and connectivity
    cmds:
      - echo "Testing IPv6 configuration..."
      - task: network:check-ipv6
      - task: bgp:verify-peering-ipv6
      - kubectl get svc -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,IPv4:.status.loadBalancer.ingress[*].ip,IPv6:.status.loadBalancer.ingress[*].ip

  test:recovery:
    desc: Test cluster recovery functionality
    cmds:
      - echo "Testing cluster recovery functionality..."
      - ./tests/test-recovery.sh

  # Maintenance tasks
  maintenance:backup:
    desc: Backup cluster configuration
    cmds:
      - mkdir -p backups/$(date +%Y%m%d-%H%M%S)
      - kubectl get all --all-namespaces -o yaml > backups/$(date +%Y%m%d-%H%M%S)/cluster-state.yaml
      - talosctl get machineconfig -o yaml > backups/$(date +%Y%m%d-%H%M%S)/talos-config.yaml

  maintenance:cleanup:
    desc: Cleanup old resources
    cmds:
      - kubectl delete pods --field-selector=status.phase=Succeeded --all-namespaces
      - kubectl delete pods --field-selector=status.phase=Failed --all-namespaces

  # Development tasks
  dev:port-forward:
    desc: Port forward common services
    cmds:
      - kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80 &
      - kubectl port-forward -n onepassword-connect svc/onepassword-connect 8081:8080 &
      - echo "Services available at localhost:8080 (Longhorn) and localhost:8081 (1Password Connect)"

  dev:logs:
    desc: Tail logs for debugging
    cmds:
      - kubectl logs -f -n kube-system -l app.kubernetes.io/name=cilium

  # Cluster status
  cluster:status:
    desc: Show comprehensive cluster status for all-control-plane cluster
    cmds:
      - echo "=== All-Control-Plane Cluster Nodes ==="
      - kubectl get nodes -o wide
      - echo ""
      - echo "=== Node Roles (should show 'control-plane' for all nodes) ==="
      - kubectl get nodes --show-labels | grep "node-role.kubernetes.io"
      - echo ""
      - echo "=== etcd Members ==="
      - kubectl get pods -n kube-system -l component=etcd -o wide
      - echo ""
      - echo "=== Control Plane Components ==="
      - kubectl get pods -n kube-system -l tier=control-plane -o wide
      - echo ""
      - echo "=== System Pods ==="
      - kubectl get pods -n kube-system
      - echo ""
      - echo "=== Longhorn Status ==="
      - kubectl get pods -n longhorn-system
      - echo ""
      - echo "=== BGP Status ==="
      - kubectl get ciliumbgpclusterconfig
      - echo ""
      - echo "=== LoadBalancer Services ==="
      - kubectl get svc --all-namespaces | grep LoadBalancer

  # Renovate tasks
  renovate:dry-run:
    desc: Run Renovate in dry-run mode to see what updates are available
    cmds:
      - echo "Running Renovate dry-run to check for available updates..."
      - RENOVATE_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") npx renovate --dry-run geoffdavis/talos-gitops
    preconditions:
      - sh: 'command -v op'
        msg: '1Password CLI (op) is required to retrieve GitHub token'

  renovate:run:
    desc: Run Renovate locally (creates actual PRs)
    cmds:
      - echo "Running Renovate to create update PRs..."
      - echo "WARNING - This will create actual PRs if updates are found"
      - echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
      - sleep 5
      - RENOVATE_TOKEN=$(op read "op://Private/GitHub Personal Access Token/token") npx renovate geoffdavis/talos-gitops
    preconditions:
      - sh: 'command -v op'
        msg: '1Password CLI (op) is required to retrieve GitHub token'

  renovate:validate:
    desc: Validate Renovate configuration
    cmds:
      - echo "Validating Renovate configuration..."
      - npx renovate-config-validator .renovaterc.json

  renovate:install:
    desc: Install Renovate CLI globally
    cmds:
      - echo "Installing Renovate CLI..."
      - npm install -g renovate